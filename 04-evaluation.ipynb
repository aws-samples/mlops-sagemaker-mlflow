{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8deea550-ecc6-4b74-be15-7654eaa6f852",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Evaluate an ML model using SageMaker\n",
    "\n",
    "![training notebook](img/sagemaker-mlops-model-evaluation-diagram.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e70d7e6-6c0d-4b91-8288-fe37a24002ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn s3fs==0.4.2 sagemaker xgboost mlflow==2.13.2 sagemaker-mlflow==0.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc15583-f1ec-4b5f-9c2d-30151c04e2d1",
   "metadata": {},
   "source": [
    "Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bedff42-6b42-4fd0-b53b-0baeb851b467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_evaluation import evaluation\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d8158d-06fd-4afc-808f-764a0f7731ca",
   "metadata": {},
   "source": [
    "# Initialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b23924-ceef-4c5b-b198-5aeb253a9426",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-east-1\"\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "sess = sagemaker.Session(boto_session=boto_session)\n",
    "bucket_name = sess.default_bucket()\n",
    "bucket_prefix = \"player-churn/xgboost\"\n",
    "notebook_param_s3_bucket_prefix=f\"s3://{bucket_name}/{bucket_prefix}/params\"\n",
    "experiment_name = \"player-churn-model-experiment\"\n",
    "mlflow_tracking_server_arn = \"\" # Provide a valid mlflow tracking server ARN. You can find the value in the output from 00-start-here.ipynb\n",
    "run_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10222553-f03e-49c1-b011-e38f458d5824",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(mlflow_tracking_server_arn) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba27b7-4004-4098-b4e0-97e48c275e4b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Interpreting the Predictions\n",
    "The actual output of many binary classification algorithms is a prediction score. The score indicates the system's certainty that the given observation belongs to the positive class (the actual target value is 1). Binary classification models output a score that ranges from 0 to 1. As a consumer of this score, to make the decision about whether the observation should be classified as 1 or 0, you interpret the score by picking a classification threshold, or cut-off, and compare the score against it. Any observations with scores higher than the cut-off are predicted as target= 1, and scores lower than the cut-off are predicted as target= 0.\n",
    "\n",
    "The default score cut-off is 0.5. You can choose to update this cut-off to match your business needs. You can use the visualizations in the console to understand how the choice of cut-off will affect your application.\n",
    "\n",
    "## Measuring ML Model Accuracy\n",
    "An industry-standard accuracy metric for binary classification models is called Area Under the (Receiver Operating Characteristic) Curve (AUC). AUC measures the ability of the model to predict a higher score for positive examples as compared to negative examples. Because it is independent of the score cut-off, you can get a sense of the prediction accuracy of your model from the AUC metric without picking a threshold.\n",
    "\n",
    "The AUC metric returns a decimal value from 0 to 1. AUC values near 1 indicate an ML model that is highly accurate. Values near 0.5 indicate an ML model that is no better than guessing at random. Values near 0 are unusual to see, and typically indicate a problem with the data. Essentially, an AUC near 0 says that the ML model has learned the correct patterns, but is using them to make predictions that are flipped from reality ('0's are predicted as '1's and vice versa). For more information about AUC, go to the [Receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) page on Wikipedia.\n",
    "\n",
    "The baseline AUC metric for a binary model is 0.5. It is the value for a hypothetical ML model that randomly predicts a 1 or 0 answer. Your binary ML model should perform better than this value to begin to be valuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5456a7aa-6933-4485-9150-b4ba6741cb6b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "auc_score = evaluation.evaluate(region,\n",
    "             bucket_prefix,\n",
    "             mlflow_tracking_server_arn,\n",
    "             experiment_name,\n",
    "             run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8436032-936d-4b03-ac66-f044d2fc4b8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Model Evaluation: AUC Score: {auc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd93ae0c-29c2-443e-94f7-b93a5a7b44e8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

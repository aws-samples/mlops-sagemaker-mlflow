{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b408ee7-8749-4989-97b1-e4951d1f5988",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Preprocessing Data for ML using SageMaker\n",
    "When working with a machine learning project, it is typical to perform pre and post processing to build the datasets specific to the model training requirements. Often times, data scientists develop preprocessing scripts in a notebook to prepare the data for training a machine learning model. For the context of this workshop, we are working with a synthetic video game dataset to train a classifier to predict the probability of player churn. The data preprocessing in this notebook focuses on curating the data structure to train an [XGBoost classifier](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html). \n",
    "\n",
    "\n",
    "![preprocessing notebook](img/sagemaker-mlops-notebook-preprocessing.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61145197-a212-484c-975f-cbda6e1f152a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install scikit-learn \"pandas>=2.0.0\" s3fs==0.4.2 sagemaker xgboost mlflow==2.13.2 sagemaker-mlflow==0.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dc82ed-f4b2-4069-a1e6-da63a6d4a344",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Import all the essential packages to be used in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d6c739-d8bc-4e51-8949-31d2805987db",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183c4301-f706-4063-b91d-58ca445c8a0b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.feature_store.feature_store import FeatureStore\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "import json\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde4b45-a0f9-4802-8f07-f818eac56ff6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9153c2fd-f7f9-47b3-b215-89787981b724",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_from_s3(s3_client, local_file_path, bucket_name, s3_file_path):\n",
    "    try:\n",
    "        # Download the file\n",
    "        s3_client.download_file(bucket_name, s3_file_path, local_file_path)\n",
    "        print(f\"File downloaded successfully to {local_file_path}\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            print(\"The object does not exist.\")\n",
    "        else:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return False\n",
    "\n",
    "def upload_to_s3(s3_client, local_file_path, bucket_name, s3_file_path=None):\n",
    "    # If S3 file path is not specified, use the basename of the local file\n",
    "    if s3_file_path is None:\n",
    "        s3_file_path = os.path.basename(local_file_path)\n",
    "\n",
    "    try:\n",
    "        # Upload the file\n",
    "        s3_client.upload_file(local_file_path, bucket_name, s3_file_path)\n",
    "        print(f\"File {local_file_path} uploaded successfully to {bucket_name}/{s3_file_path}\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"ClientError: {e}\")\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {local_file_path} was not found\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return False\n",
    "        \n",
    "def write_params(s3_client, step_name, params, notebook_param_s3_bucket_prefix):\n",
    "    local_file_path = f\"{step_name}.json\"\n",
    "    with open(local_file_path, \"w\") as f:\n",
    "        f.write(json.dumps(params))\n",
    "    base_local_file_path = os.path.basename(local_file_path)\n",
    "    bucket_name = notebook_param_s3_bucket_prefix.split(\"/\")[2] # Format: s3://<bucket_name>/..\n",
    "    s3_file_path = os.path.join(\"/\".join(notebook_param_s3_bucket_prefix.split(\"/\")[3:]), base_local_file_path)\n",
    "    upload_to_s3(s3_client, local_file_path, bucket_name, s3_file_path)\n",
    "    \n",
    "def read_params(s3_client, notebook_param_s3_bucket_prefix, step_name):\n",
    "    local_file_path = f\"{step_name}.json\"\n",
    "    base_local_file_path = os.path.basename(local_file_path)\n",
    "    bucket_name = notebook_param_s3_bucket_prefix.split(\"/\")[2] # Format: s3://<bucket_name>/..\n",
    "    s3_file_path = os.path.join(\"/\".join(notebook_param_s3_bucket_prefix.split(\"/\")[3:]),  base_local_file_path)\n",
    "    downloaded = download_from_s3(s3_client, local_file_path, bucket_name, s3_file_path)\n",
    "    with open(local_file_path, \"r\") as f:\n",
    "        data = f.read()\n",
    "        params = json.loads(data)\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cbf077-1563-4085-9d73-f04a57c76e6a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Define constants for the preprocessing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b68da-6243-4a23-91e2-6fee63d6db80",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_col = \"player_churn\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce4d47-321a-42cc-8038-15ff3c5f916c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Initializes Variables / Parameters\n",
    "The following variables are defined in this cell specifically used throughout the notebook. In addition to the hardcoded values, these variables can also be passed into the notebook as parameters when the notebook is scheduled to run remotely, such as a SageMaker Pipeline job. We'll dive into how to pass parameters into this notebook in the next lab. The variables defined in the following cell can be updated with different values when scheduled via CICD pipeline through SageMaker Project. Please refer to [this](https://docs.aws.amazon.com/sagemaker/latest/dg/notebook-auto-run-troubleshoot-override.html) documentation for more information notebook parameterization.\n",
    "\n",
    "Specifically, there are 2 variables that we need to obtain in order to run this notebook successfully in SageMaker Studio. \n",
    "\n",
    "* feature_group_name\n",
    "* mlflow_tracking_server_arn\n",
    "\n",
    "\n",
    "The following section describes how to obtain these variables through SageMaker Studio Launcher.\n",
    "\n",
    "### Feature Store Group\n",
    "If you have completed the feature-store ingestion lab (lab1), there should be a SageMaker feature group created. To obtain the feature group name, navigate to SageMaker Studio launcher, then select `Data` dropdown from the left pane, select `Feature Store` and find the appropriate Feature Store Group name on the right pane, as depicted in the following diagram:\n",
    "\n",
    "![studio feature group console](img/sagemaker-studio-feature-group-console.jpg)\n",
    "\n",
    "### MLFlow Tracking Server¶\n",
    "Machine learning is an iterative process that requires experimenting with various combinations of data, algorithms, and parameters, while observing their impact on model accuracy. The iterative nature of ML experimentation results in numerous model training runs and versions, making it challenging to track the best performing models and their configurations. The complexity of managing and comparing iterative training runs increases with generative artificial intelligence (generative AI), where experimentation involves not only fine-tuning models but also exploring creative and diverse outputs. Researchers must adjust hyperparameters, select suitable model architectures, and curate diverse datasets to optimize both the quality and creativity of the generated content. Evaluating generative AI models requires both quantitative and qualitative metrics, adding another layer of complexity to the experimentation process.\n",
    "\n",
    "Throughout the workshop, you'll integrate MLflow with Amazon SageMaker to track, organize, view, analyze, and compare iterative ML experimentation to gain comparative insights and register and deploy your best performing models.\n",
    "\n",
    "Please refer to this documentation to learn more about SageMaker MLFlow integration.\n",
    "\n",
    "\n",
    "If you completed the `00-start-here.ipynb` prior to this notebook, there should be an MLFlow tracking server provisioned for you to use. To get the ARN for the tracking server, go to the SageMaker Studio Launcher, select `MLFlow` from the Application list on the top left corner of the launcher, select the appropriate tracking server, and find the ARN under the `Configuration` section, as depicted in the following diagram:\n",
    "\n",
    "![mlflow tracking](img/sagemaker-studio-launcher-mlflow-console.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74b56af-3926-4892-abaa-77d6a76f6ee1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "region = \"us-east-1\"\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = region\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "# Sagemaker session\n",
    "sess = sagemaker.Session(boto_session=boto_session)\n",
    "bucket_name = sess.default_bucket()\n",
    "sagemaker_client = boto_session.client(service_name='sagemaker', region_name=region)\n",
    "featurestore_runtime = boto_session.client(service_name='sagemaker-featurestore-runtime', region_name=region)\n",
    "bucket_prefix = \"player-churn/xgboost\"\n",
    "notebook_param_s3_bucket_prefix=f\"s3://{bucket_name}/{bucket_prefix}/params\"\n",
    "experiment_name = \"player-churn-model-experiment\"\n",
    "feature_group_name = \"\" # Replace the feature group name created in the previous lab\n",
    "mlflow_tracking_server_arn = \"\" # Provide a valid mlflow tracking server ARN. You can find the value in the output from 00-start-here.ipynb\n",
    "run_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f86a48-a9ad-4337-922b-e3c777f1a44e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(feature_group_name) > 0\n",
    "assert len(mlflow_tracking_server_arn) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d8db8d-9341-4af8-ba07-c034e3334c70",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The following cell integrates MLFlow tracking server with this preprocessing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c3164-4cf7-4829-bea6-5224b8fae9db",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "suffix = strftime('%d-%H-%M-%S', gmtime())\n",
    "mlflow.set_tracking_uri(mlflow_tracking_server_arn)\n",
    "experiment = mlflow.set_experiment(experiment_name=experiment_name)\n",
    "run = mlflow.start_run(run_id=run_id) if run_id else mlflow.start_run(run_name=f\"processing-{suffix}\", nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e7497-31c7-444f-bcdc-50a6acd98b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FeatureStore session object\n",
    "feature_store_session = Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime,\n",
    ")\n",
    "\n",
    "feature_store = FeatureStore(sagemaker_session=feature_store_session)\n",
    "dataset_feature_group = FeatureGroup(feature_group_name)\n",
    "\n",
    "query_output_s3_path = f's3://{bucket_name}/data/query_results/'\n",
    "# Create dataset builder to retrieve the most recent version of each record\n",
    "builder = feature_store.create_dataset(\n",
    "    base=dataset_feature_group,\n",
    "    # included_feature_names=inlcuded_feature_names,\n",
    "    output_path=query_output_s3_path,\n",
    ").with_number_of_recent_records_by_record_identifier(1)\n",
    "\n",
    "player_churn_fs_df, query = builder.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701de184-872a-4821-a755-4388afbcba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_churn_fs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc52d259-8961-4f4d-9d48-6fb8e481ee05",
   "metadata": {},
   "source": [
    "# Data Processing and Splitting Train Data \n",
    "In the previous step, we extracted the required dataset from SageMaker Feature Store for training a model. For our use case, we'll leverage a powerful and efficient ML algorithm called  [XGBoost](https://xgboost.readthedocs.io/en/stable/) to train a classifier to predict player churn. The data structure for XGBoost can be found [here](https://xgboost.readthedocs.io/en/stable/tutorials/input_format.html#id1). In the following section, we'll format the dataset to meet the requirements for XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc979b-8907-4710-9607-f822a724d8d4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removing the columns from Feature Store not relevant to training a model.\n",
    "df_model_data = player_churn_fs_df.drop( columns = [\"player_id\", \"event_time\"])\n",
    "\n",
    "# Reorganize the data structure to make the label column as the first column in the dataset.\n",
    "df_model_data = pd.concat([df_model_data[\"player_churn\"], df_model_data.drop(columns=[\"player_churn\"])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf08bca0-fa97-47d3-b3a0-2e6b8a581358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and splitting dataset\n",
    "train_data, validation_data, test_data = np.split(\n",
    "    df_model_data.sample(frac=1, random_state=1729),\n",
    "    [int(0.7 * len(df_model_data)), int(0.9 * len(df_model_data))],\n",
    ")\n",
    "\n",
    "print(f\"Data split > train:{train_data.shape} | validation:{validation_data.shape} | test:{test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e6e601-1fb7-4dd5-a523-778e7b8111bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_output_s3_path = f\"s3://{bucket_name}/{bucket_prefix}/train/train.csv\"\n",
    "validation_data_output_s3_path = f\"s3://{bucket_name}/{bucket_prefix}/validation/validation.csv\"\n",
    "test_x_data_output_s3_path = f\"s3://{bucket_name}/{bucket_prefix}/test/test_x.csv\"\n",
    "test_y_data_output_s3_path = f\"s3://{bucket_name}/{bucket_prefix}/test/test_y.csv\"\n",
    "baseline_data_output_s3_path = f\"s3://{bucket_name}/{bucket_prefix}/baseline/baseline.csv\"\n",
    "\n",
    "train_data.to_csv(train_data_output_s3_path, index=False, header=False)\n",
    "validation_data.to_csv(validation_data_output_s3_path, index=False, header=False)\n",
    "test_data[target_col].to_csv(test_y_data_output_s3_path, index=False, header=False)\n",
    "test_data.drop([target_col], axis=1).to_csv(test_x_data_output_s3_path, index=False, header=False)\n",
    "\n",
    "# We need the baseline dataset for model monitoring\n",
    "df_model_data.drop([target_col], axis=1).to_csv(baseline_data_output_s3_path, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbff1ef-eb45-4f9d-9942-0cc9228c496d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Store Parameters\n",
    "In the following cell, we'll store the relevant parameters to S3 bucket so that they could be passed to other steps in the subsequent steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed29300a-0506-4701-99f5-493b77e314c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['train_data'] = train_data_output_s3_path\n",
    "params['validation_data'] = validation_data_output_s3_path\n",
    "params['test_x_data'] = test_x_data_output_s3_path\n",
    "params['test_y_data'] = test_y_data_output_s3_path\n",
    "params['baseline_data'] = baseline_data_output_s3_path\n",
    "params['experiment_name'] = experiment.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ee2e5-ac29-4906-bd9f-3752a88ae1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "step_name = \"02-preprocess\"\n",
    "write_params(s3_client, step_name, params, notebook_param_s3_bucket_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f585e-c0a5-42f0-8ec8-ed8c93bd6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7cbaa6-786a-48d0-bb01-322f57579246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

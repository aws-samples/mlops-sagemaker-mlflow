{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow 3.3+ Prompt Management with Amazon Bedrock Converse API\n",
    "\n",
    "This notebook demonstrates how to use MLflow 3.3+'s prompt management features with Amazon Bedrock models using the Converse API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages - MLflow 3.3+ with enhanced prompt management\n",
    "!pip install \"mlflow>=3.3.0\" \"boto3>=1.34.0\" \"botocore>=1.34.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve ml tracking server arn\n",
    "%store -r tracking_server_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import mlflow\n",
    "import boto3\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "print(f\"Boto3 version: {boto3.__version__}\")\n",
    "\n",
    "# MLflow 3.3+ uses register_prompt and load_prompt for prompt management\n",
    "print(\"\\n‚úÖ MLflow 3.3+ prompt management functions available:\")\n",
    "print(\"- mlflow.register_prompt()\")\n",
    "print(\"- mlflow.load_prompt()\")\n",
    "print(\"- mlflow.search_prompts()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "print(f\"Using AWS Region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bedrock client\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name=region)\n",
    "\n",
    "# Set MLflow tracking URI\n",
    "mlflow.set_tracking_uri(tracking_server_arn) \n",
    "\n",
    "print(\"‚úÖ AWS Bedrock client initialized\")\n",
    "print(\"‚úÖ MLflow 3.3+ tracking configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new experiment\n",
    "experiment_name = \"bedrock-prompt-management\"\n",
    "try:\n",
    "    experiment_id = mlflow.create_experiment(experiment_name)\n",
    "    print(f\"Created new experiment: {experiment_name}\")\n",
    "except mlflow.exceptions.MlflowException:\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    experiment_id = experiment.experiment_id\n",
    "    print(f\"Using existing experiment: {experiment_name}\")\n",
    "\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Register Prompt Template (MLflow 3.3+ API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a prompt template using MLflow 3.3+ API\n",
    "print(\"Registering customer support prompt template...\")\n",
    "\n",
    "try:\n",
    "    # Note: MLflow 3.3+ uses {{variable}} syntax (double curly braces)\n",
    "    prompt_template = \"You are a helpful customer support agent. \"\\\n",
    "                     \"Customer question: {{question}}\\n\"\\\n",
    "                     \"Product context: {{product_info}}\\n\"\\\n",
    "                     \"Please provide a helpful and professional response.\"\n",
    "    \n",
    "    customer_support_prompt = mlflow.register_prompt(\n",
    "        name=\"customer-support-prompt\",\n",
    "        template=prompt_template,\n",
    "        commit_message=\"Initial customer support prompt template\",\n",
    "        tags={\"use_case\": \"customer_support\", \"version\": \"v1\"}\n",
    "    )\n",
    "    print(f\"‚úÖ Registered prompt: {customer_support_prompt.name} (version {customer_support_prompt.version})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Prompt registration error: {e}\")\n",
    "    print(\"This might be expected if the prompt already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use Prompt Template with Bedrock Converse API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the prompt template with Bedrock Converse API directly\n",
    "\n",
    "with mlflow.start_run(run_name=\"basic_customer_support\"):\n",
    "    # Load the prompt template using MLflow 3.3+ API\n",
    "    try:\n",
    "        prompt = mlflow.genai.load_prompt(\"customer-support-prompt\", version=1)\n",
    "        print(f\"Loaded prompt: {prompt.name} (version {prompt.version})\")\n",
    "        \n",
    "        # Fill in the template variables using MLflow 3.3+ format method\n",
    "        filled_prompt = prompt.format(\n",
    "            question=\"How do I return a defective product?\",\n",
    "            product_info=\"Electronics with 30-day return policy\"\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading prompt: {e}\")\n",
    "        # Fallback to manual template\n",
    "        filled_prompt = \"You are a helpful customer support agent. \"\\\n",
    "                       \"Customer question: How do I return a defective product?\\n\"\\\n",
    "                       \"Product context: Electronics with 30-day return policy\\n\"\\\n",
    "                       \"Please provide a helpful and professional response.\"\n",
    "    \n",
    "    print(f\"Filled prompt:\\n{filled_prompt}\\n\")\n",
    "    \n",
    "    # Log parameters\n",
    "    model_id = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "    mlflow.log_param(\"model_id\", model_id)\n",
    "    mlflow.log_param(\"prompt_name\", \"customer-support-prompt\")\n",
    "    mlflow.log_param(\"max_tokens\", 1000)\n",
    "    mlflow.log_param(\"temperature\", 0.7)\n",
    "    \n",
    "    # Log the prompt\n",
    "    mlflow.log_text(filled_prompt, \"input_prompt.txt\")\n",
    "    \n",
    "    # Invoke Bedrock model using Converse API directly\n",
    "    try:\n",
    "        response = bedrock_runtime.converse(\n",
    "            modelId=model_id,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"text\": filled_prompt\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": 1000,\n",
    "                \"temperature\": 0.7,\n",
    "                \"topP\": 0.9\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Extract response text\n",
    "        response_text = response['output']['message']['content'][0]['text']\n",
    "        usage = response.get('usage', {})\n",
    "        stop_reason = response['stopReason']\n",
    "        \n",
    "        # Log response and metrics\n",
    "        mlflow.log_text(response_text, \"model_response.txt\")\n",
    "        mlflow.log_metric(\"response_length\", len(response_text))\n",
    "        mlflow.log_metric(\"prompt_length\", len(filled_prompt))\n",
    "        mlflow.log_param(\"stop_reason\", stop_reason)\n",
    "        \n",
    "        # Log token usage if available\n",
    "        if usage:\n",
    "            mlflow.log_metric(\"input_tokens\", usage.get(\"inputTokens\", 0))\n",
    "            mlflow.log_metric(\"output_tokens\", usage.get(\"outputTokens\", 0))\n",
    "            mlflow.log_metric(\"total_tokens\", usage.get(\"totalTokens\", 0))\n",
    "        \n",
    "        print(f\"Model response:\\n{response_text}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error invoking Bedrock model: {e}\"\n",
    "        print(error_msg)\n",
    "        mlflow.log_text(error_msg, \"error_log.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. System Prompt Example -  we will examine different metrics that can be tracked in MLFlow for evaluating prompt performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a system prompt template\n",
    "try:\n",
    "    system_prompt_template = mlflow.genai.register_prompt(\n",
    "        name=\"teaching-system-prompt\",\n",
    "        template=\"You are an excellent teacher who explains complex concepts in simple, engaging ways. \"\\\n",
    "                \"Always use analogies and examples that a {{age_group}} can understand.\",\n",
    "        commit_message=\"System prompt for teaching complex topics\",\n",
    "        tags={\"type\": \"system_prompt\", \"domain\": \"education\"}\n",
    "    )\n",
    "    print(f\"‚úÖ Registered system prompt: {system_prompt_template.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"System prompt registration: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with system prompt\n",
    "with mlflow.start_run(run_name=\"system_prompt_example\"):\n",
    "    user_message = \"Explain machine learning in simple terms.\"\n",
    "    \n",
    "    # Load and format system prompt\n",
    "    try:\n",
    "        system_prompt_template = mlflow.genai.load_prompt(\"teaching-system-prompt\", version=1)\n",
    "        system_prompt = system_prompt_template.format(age_group=\"12-year-old\")\n",
    "    except Exception as e:\n",
    "        print(f\"Using fallback system prompt: {e}\")\n",
    "        system_prompt = \"You are an excellent teacher who explains complex concepts simply.\"\n",
    "    \n",
    "    # Log parameters\n",
    "    model_id = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "    mlflow.log_param(\"model_id\", model_id)\n",
    "    mlflow.log_param(\"has_system_prompt\", True)\n",
    "    mlflow.log_param(\"system_prompt_name\", \"teaching-system-prompt\")\n",
    "    \n",
    "    # Log prompts\n",
    "    mlflow.log_text(system_prompt, \"system_prompt.txt\")\n",
    "    mlflow.log_text(user_message, \"user_message.txt\")\n",
    "    \n",
    "    # Get response using Converse API with system prompt\n",
    "    try:\n",
    "        response = bedrock_runtime.converse(\n",
    "            modelId=model_id,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"text\": user_message\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            system=[\n",
    "                {\n",
    "                    \"text\": system_prompt\n",
    "                }\n",
    "            ],\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": 1500,\n",
    "                \"temperature\": 0.7,\n",
    "                \"topP\": 0.9\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Extract response details\n",
    "        response_text = response['output']['message']['content'][0]['text']\n",
    "        usage = response.get('usage', {})\n",
    "        \n",
    "        # Log response and metrics\n",
    "        mlflow.log_text(response_text, \"model_response.txt\")\n",
    "        mlflow.log_metric(\"response_length\", len(response_text))\n",
    "        \n",
    "        if usage:\n",
    "            mlflow.log_metric(\"input_tokens\", usage.get(\"inputTokens\", 0))\n",
    "            mlflow.log_metric(\"output_tokens\", usage.get(\"outputTokens\", 0))\n",
    "            mlflow.log_metric(\"total_tokens\", usage.get(\"totalTokens\", 0))\n",
    "        \n",
    "        print(f\"System Prompt Response:\\n{response_text}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error with system prompt: {e}\"\n",
    "        print(error_msg)\n",
    "        mlflow.log_text(error_msg, \"error_log.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Search and Manage Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for registered prompts\n",
    "print(\"Searching for registered prompts...\")\n",
    "try:\n",
    "    prompts = mlflow.genai.search_prompts()\n",
    "    print(f\"Found {len(prompts)} registered prompts:\")\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"- {prompt.name}\")\n",
    "        if hasattr(prompt, 'description') and prompt.description:\n",
    "            print(f\"  Description: {prompt.description}\")\n",
    "        if hasattr(prompt, 'tags') and prompt.tags:\n",
    "            print(f\"  Tags: {prompt.tags}\")\n",
    "        \n",
    "        # To get version info, we need to load the prompt\n",
    "        try:\n",
    "            loaded_prompt = mlflow.genai.load_prompt(prompt.name, version=1)\n",
    "            print(f\"  Latest version: {loaded_prompt.version}\")\n",
    "            print(f\"  Template preview: {loaded_prompt.template[:100]}...\")\n",
    "        except Exception as load_error:\n",
    "            print(f\"  Could not load version info: {load_error}\")\n",
    "        print()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error searching prompts: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get experiment information\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "print(f\"Experiment ID: {experiment.experiment_id}\")\n",
    "print(f\"Experiment Name: {experiment.name}\")\n",
    "\n",
    "# Get all runs from the experiment\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "print(f\"\\nTotal runs in experiment: {len(runs)}\")\n",
    "\n",
    "# Display summary of runs\n",
    "if len(runs) > 0:\n",
    "    print(\"\\nRun Summary:\")\n",
    "    for idx, run in runs.iterrows():\n",
    "        print(f\"- {run['tags.mlflow.runName']}: {run['status']}\")\n",
    "        if 'metrics.total_tokens' in run and run['metrics.total_tokens'] is not None:\n",
    "            print(f\"  Total tokens: {int(run['metrics.total_tokens'])}\")\n",
    "        if 'metrics.response_length' in run and run['metrics.response_length'] is not None:\n",
    "            print(f\"  Response length: {int(run['metrics.response_length'])} chars\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Versions - this will help us store and compare multiple versions of prompts against the same model. We can then choose which version of the prompt we use in our model invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create version 2 of the customer support prompt with improvements\n",
    "print(\"Creating version 2 of customer support prompt...\")\n",
    "\n",
    "try:\n",
    "    # Enhanced prompt template with more structure and empathy\n",
    "    enhanced_prompt_template = \"\"\"You are an expert customer support specialist with 10+ years of experience.\n",
    "    \n",
    "Customer Information:\n",
    "- Question: {{question}}\n",
    "- Product Context: {{product_info}}\n",
    "- Customer Tier: {{customer_tier}}\n",
    "- Urgency Level: {{urgency}}\n",
    "\n",
    "Instructions:\n",
    "1. Acknowledge the customer's concern with empathy\n",
    "2. Provide a clear, step-by-step solution\n",
    "3. Offer additional resources or escalation if needed\n",
    "4. End with a professional closing and next steps\n",
    "\n",
    "Please provide a comprehensive, helpful response.\"\"\"\n",
    "    \n",
    "    # Register the enhanced version (this will create version 2)\n",
    "    customer_support_v2 = mlflow.genai.register_prompt(\n",
    "        name=\"customer-support-prompt\",  # Same name creates new version\n",
    "        template=enhanced_prompt_template,\n",
    "        commit_message=\"Enhanced customer support prompt with structured approach and empathy\",\n",
    "        tags={\"use_case\": \"customer_support\", \"version\": \"v2\", \"enhancement\": \"structured_empathetic\"}\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Created version 2: {customer_support_v2.name} (version {customer_support_v2.version})\")\n",
    "    print(f\"Commit message: {customer_support_v2.commit_message}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating version 2: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compare both versions of the prompt\n",
    "print(\"Comparing prompt versions...\\n\")\n",
    "\n",
    "try:\n",
    "    # Load version 1\n",
    "    prompt_v1 = mlflow.genai.load_prompt(\"customer-support-prompt\", version=1)\n",
    "    print(f\"üìã VERSION 1 (Original):\")\n",
    "    print(f\"Template length: {len(prompt_v1.template)} characters\")\n",
    "    print(f\"Template preview: {prompt_v1.template[:150]}...\\n\")\n",
    "    \n",
    "    # Load version 2\n",
    "    prompt_v2 = mlflow.genai.load_prompt(\"customer-support-prompt\", version=2)\n",
    "    print(f\"üìã VERSION 2 (Enhanced):\")\n",
    "    print(f\"Template length: {len(prompt_v2.template)} characters\")\n",
    "    print(f\"Template preview: {prompt_v2.template[:150]}...\\n\")\n",
    "    \n",
    "    # Show the difference in structure\n",
    "    print(f\"üìä COMPARISON:\")\n",
    "    print(f\"Length difference: {len(prompt_v2.template) - len(prompt_v1.template)} characters\")\n",
    "    print(f\"V1 Variables: {len(prompt_v1.template.split('{{'))-1} variables\")\n",
    "    print(f\"V2 Variables: {len(prompt_v2.template.split('{{'))-1} variables\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error comparing versions: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B test both prompt versions with the same input\n",
    "import time\n",
    "\n",
    "# Test scenario\n",
    "test_scenario = {\n",
    "    \"question\": \"My laptop arrived damaged and I need a replacement urgently for work\",\n",
    "    \"product_info\": \"Dell XPS 13 laptop, Premium warranty, ordered 3 days ago\",\n",
    "    \"customer_tier\": \"Premium\",\n",
    "    \"urgency\": \"High\"\n",
    "}\n",
    "\n",
    "print(\"üß™ A/B Testing Prompt Versions...\\n\")\n",
    "print(f\"Test Scenario: {test_scenario['question']}\\n\")\n",
    "\n",
    "model_id = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "results = {}\n",
    "\n",
    "# Test both versions\n",
    "for version in [1, 2]:\n",
    "    with mlflow.start_run(run_name=f\"customer_support_v{version}_ab_test\"):\n",
    "        try:\n",
    "            # Load prompt version\n",
    "            prompt = mlflow.genai.load_prompt(\"customer-support-prompt\", version=version)\n",
    "            \n",
    "            # Format prompt (V1 uses fewer variables)\n",
    "            if version == 1:\n",
    "                filled_prompt = prompt.format(\n",
    "                    question=test_scenario[\"question\"],\n",
    "                    product_info=test_scenario[\"product_info\"]\n",
    "                )\n",
    "            else:\n",
    "                filled_prompt = prompt.format(**test_scenario)\n",
    "            \n",
    "            # Log parameters\n",
    "            mlflow.log_param(\"prompt_version\", f\"v{version}\")\n",
    "            mlflow.log_param(\"model_id\", model_id)\n",
    "            mlflow.log_param(\"test_type\", \"ab_test\")\n",
    "            mlflow.log_text(filled_prompt, f\"prompt_v{version}.txt\")\n",
    "            \n",
    "            # Call Bedrock\n",
    "            start_time = time.time()\n",
    "            response = bedrock_runtime.converse(\n",
    "                modelId=model_id,\n",
    "                messages=[{\"role\": \"user\", \"content\": [{\"text\": filled_prompt}]}],\n",
    "                inferenceConfig={\"maxTokens\": 1500, \"temperature\": 0.7, \"topP\": 0.9}\n",
    "            )\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            # Extract results\n",
    "            response_text = response['output']['message']['content'][0]['text']\n",
    "            usage = response.get('usage', {})\n",
    "            \n",
    "            # Log metrics\n",
    "            mlflow.log_text(response_text, f\"response_v{version}.txt\")\n",
    "            mlflow.log_metric(\"response_time_seconds\", response_time)\n",
    "            mlflow.log_metric(\"response_length\", len(response_text))\n",
    "            mlflow.log_metric(\"response_word_count\", len(response_text.split()))\n",
    "            mlflow.log_metric(\"prompt_length\", len(filled_prompt))\n",
    "            \n",
    "            if usage:\n",
    "                mlflow.log_metric(\"total_tokens\", usage.get(\"totalTokens\", 0))\n",
    "                mlflow.log_metric(\"input_tokens\", usage.get(\"inputTokens\", 0))\n",
    "                mlflow.log_metric(\"output_tokens\", usage.get(\"outputTokens\", 0))\n",
    "            \n",
    "            # Store results for comparison\n",
    "            results[f\"v{version}\"] = {\n",
    "                \"response_text\": response_text,\n",
    "                \"response_time\": response_time,\n",
    "                \"response_length\": len(response_text),\n",
    "                \"word_count\": len(response_text.split()),\n",
    "                \"total_tokens\": usage.get(\"totalTokens\", 0)\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ VERSION {version} COMPLETED:\")\n",
    "            print(f\"   Response: {len(response_text)} chars, {len(response_text.split())} words\")\n",
    "            print(f\"   Time: {response_time:.2f}s, Tokens: {usage.get('totalTokens', 0)}\")\n",
    "            print(f\"   Preview: {response_text[:200]}...\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error testing V{version}: {e}\")\n",
    "            mlflow.log_text(str(e), f\"error_v{version}.txt\")\n",
    "\n",
    "# Compare results\n",
    "if len(results) == 2:\n",
    "    print(\"üìä A/B TEST COMPARISON:\")\n",
    "    print(f\"{'Metric':<20} {'V1':<12} {'V2':<12} {'Difference':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    v1, v2 = results[\"v1\"], results[\"v2\"]\n",
    "    \n",
    "    metrics = [\n",
    "        (\"Response Length\", \"response_length\"),\n",
    "        (\"Word Count\", \"word_count\"),\n",
    "        (\"Total Tokens\", \"total_tokens\"),\n",
    "        (\"Response Time\", \"response_time\")\n",
    "    ]\n",
    "    \n",
    "    for name, key in metrics:\n",
    "        v1_val = v1[key]\n",
    "        v2_val = v2[key]\n",
    "        diff = v2_val - v1_val\n",
    "        pct = (diff / v1_val * 100) if v1_val > 0 else 0\n",
    "        print(f\"{name:<20} {v1_val:<12.2f} {v2_val:<12.2f} {diff:+.2f} ({pct:+.1f}%)\")\n",
    "    \n",
    "    print(\"\\nüí° Key Insights:\")\n",
    "    if v2[\"response_length\"] > v1[\"response_length\"]:\n",
    "        print(\"‚Ä¢ V2 generates more detailed responses\")\n",
    "    if v2[\"total_tokens\"] > v1[\"total_tokens\"]:\n",
    "        print(\"‚Ä¢ V2 uses more tokens (higher cost, potentially better quality)\")\n",
    "    print(\"‚Ä¢ Compare response quality manually to determine the winner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Models  - this will help us evaluate how two different models perform with the same prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple model evaluation setup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üî¨ Setting up simple model evaluation...\")\n",
    "\n",
    "# Models to compare\n",
    "models_to_compare = [\n",
    "    {\n",
    "        \"name\": \"Claude 3 Sonnet\",\n",
    "        \"model_id\": \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "        \"generation\": \"3\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Claude 4 Sonnet\",\n",
    "        \"model_id\": \"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "        \"generation\": \"4\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Simple test prompt\n",
    "test_prompt = \"\"\"You are a helpful customer support agent.\n",
    "\n",
    "Customer question: I'm having trouble with my subscription renewal. It keeps failing when I try to update my payment method.\n",
    "\n",
    "Please provide a helpful solution with clear steps.\"\"\"\n",
    "\n",
    "print(f\"‚úÖ Configured {len(models_to_compare)} models for evaluation\")\n",
    "print(f\"‚úÖ Test prompt ready ({len(test_prompt)} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models with the same prompt\n",
    "print(\"ü§ñ Evaluating models...\\n\")\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for model in models_to_compare:\n",
    "    print(f\"Testing {model['name']}...\")\n",
    "    \n",
    "    # Create MLflow run for this model evaluation\n",
    "    with mlflow.start_run(run_name=f\"eval_{model['name'].replace(' ', '_').lower()}\"):\n",
    "        try:\n",
    "            # Log model information\n",
    "            mlflow.log_param(\"model_name\", model[\"name\"])\n",
    "            mlflow.log_param(\"model_id\", model[\"model_id\"])\n",
    "            mlflow.log_param(\"model_generation\", model[\"generation\"])\n",
    "            mlflow.log_param(\"evaluation_type\", \"simple_model_comparison\")\n",
    "            \n",
    "            # Log the test prompt\n",
    "            mlflow.log_text(test_prompt, \"test_prompt.txt\")\n",
    "            \n",
    "            # Call Bedrock with timing\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = bedrock_runtime.converse(\n",
    "                modelId=model[\"model_id\"],\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"text\": test_prompt\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                inferenceConfig={\n",
    "                    \"maxTokens\": 1000,\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"topP\": 0.9\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            # Extract response details\n",
    "            response_text = response['output']['message']['content'][0]['text']\n",
    "            usage = response.get('usage', {})\n",
    "            stop_reason = response['stopReason']\n",
    "            \n",
    "            # Calculate simple metrics\n",
    "            word_count = len(response_text.split())\n",
    "            char_count = len(response_text)\n",
    "            sentence_count = len([s for s in response_text.split('.') if s.strip()])\n",
    "            \n",
    "            # Log metrics to MLflow\n",
    "            mlflow.log_metric(\"response_time_seconds\", response_time)\n",
    "            mlflow.log_metric(\"response_word_count\", word_count)\n",
    "            mlflow.log_metric(\"response_char_count\", char_count)\n",
    "            mlflow.log_metric(\"sentence_count\", sentence_count)\n",
    "            mlflow.log_param(\"stop_reason\", stop_reason)\n",
    "            \n",
    "            # Log token usage if available\n",
    "            if usage:\n",
    "                input_tokens = usage.get(\"inputTokens\", 0)\n",
    "                output_tokens = usage.get(\"outputTokens\", 0)\n",
    "                total_tokens = usage.get(\"totalTokens\", 0)\n",
    "                \n",
    "                mlflow.log_metric(\"input_tokens\", input_tokens)\n",
    "                mlflow.log_metric(\"output_tokens\", output_tokens)\n",
    "                mlflow.log_metric(\"total_tokens\", total_tokens)\n",
    "                \n",
    "                # Simple cost estimate (approximate)\n",
    "                estimated_cost = (input_tokens * 0.003 + output_tokens * 0.015) / 1000\n",
    "                mlflow.log_metric(\"estimated_cost_usd\", estimated_cost)\n",
    "            \n",
    "            # Log the full response\n",
    "            mlflow.log_text(response_text, \"model_response.txt\")\n",
    "            \n",
    "            # Store results for comparison\n",
    "            result = {\n",
    "                \"model_name\": model[\"name\"],\n",
    "                \"generation\": model[\"generation\"],\n",
    "                \"response_time\": response_time,\n",
    "                \"word_count\": word_count,\n",
    "                \"char_count\": char_count,\n",
    "                \"total_tokens\": total_tokens if usage else 0,\n",
    "                \"estimated_cost\": estimated_cost if usage else 0,\n",
    "                \"response_preview\": response_text[:150] + \"...\"\n",
    "            }\n",
    "            evaluation_results.append(result)\n",
    "            \n",
    "            print(f\"  ‚úÖ Response: {word_count} words, {response_time:.2f}s, {total_tokens if usage else 0} tokens\")\n",
    "            print(f\"  Preview: {response_text[:100]}...\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error evaluating {model['name']}: {e}\"\n",
    "            print(f\"  ‚ùå {error_msg}\")\n",
    "            mlflow.log_text(error_msg, \"evaluation_error.txt\")\n",
    "            \n",
    "            # Log error result\n",
    "            result = {\n",
    "                \"model_name\": model[\"name\"],\n",
    "                \"generation\": model[\"generation\"],\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            evaluation_results.append(result)\n",
    "\n",
    "print(f\"‚úÖ Evaluation completed with {len(evaluation_results)} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the tracking server url\n",
    "s = boto3.client(\"sagemaker\").list_mlflow_tracking_servers(TrackingServerStatus='Created')\n",
    "tracking_server_name = s['TrackingServerSummaries'][0]['TrackingServerName']\n",
    "\n",
    "u = boto3.client(\"sagemaker\").describe_mlflow_tracking_server(TrackingServerName=tracking_server_name)\n",
    "tracking_server_url = u['TrackingServerUrl']\n",
    "\n",
    "print(tracking_server_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open SageMaker MLFlow UI and see the trace logged under the traces tab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

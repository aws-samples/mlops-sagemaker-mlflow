{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow 3.3+ Prompt Management with Amazon Bedrock Converse API\n",
    "\n",
    "This notebook demonstrates how to use MLflow 3.3+'s prompt management features with Amazon Bedrock models using the Converse API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T15:37:14.575168Z",
     "iopub.status.busy": "2025-09-08T15:37:14.574896Z",
     "iopub.status.idle": "2025-09-08T15:37:14.577851Z",
     "shell.execute_reply": "2025-09-08T15:37:14.577301Z",
     "shell.execute_reply.started": "2025-09-08T15:37:14.575149Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages - MLflow 3.3+ with enhanced prompt management\n",
    "# !pip install \"mlflow>=3.3.0\" \"boto3>=1.34.0\" \"botocore>=1.34.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T15:37:17.000330Z",
     "iopub.status.busy": "2025-09-08T15:37:17.000066Z",
     "iopub.status.idle": "2025-09-08T15:37:17.735791Z",
     "shell.execute_reply": "2025-09-08T15:37:17.735109Z",
     "shell.execute_reply.started": "2025-09-08T15:37:17.000311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow version: 3.3.2\n",
      "Boto3 version: 1.38.46\n",
      "\n",
      "‚úÖ MLflow 3.3+ prompt management functions available:\n",
      "- mlflow.register_prompt()\n",
      "- mlflow.load_prompt()\n",
      "- mlflow.search_prompts()\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import mlflow\n",
    "import boto3\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "print(f\"Boto3 version: {boto3.__version__}\")\n",
    "\n",
    "# MLflow 3.3+ uses register_prompt and load_prompt for prompt management\n",
    "print(\"\\n‚úÖ MLflow 3.3+ prompt management functions available:\")\n",
    "print(\"- mlflow.register_prompt()\")\n",
    "print(\"- mlflow.load_prompt()\")\n",
    "print(\"- mlflow.search_prompts()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T15:37:33.299898Z",
     "iopub.status.busy": "2025-09-08T15:37:33.299626Z",
     "iopub.status.idle": "2025-09-08T15:37:33.371588Z",
     "shell.execute_reply": "2025-09-08T15:37:33.370946Z",
     "shell.execute_reply.started": "2025-09-08T15:37:33.299877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AWS Bedrock client initialized\n",
      "‚úÖ MLflow 3.3+ tracking configured\n"
     ]
    }
   ],
   "source": [
    "# Initialize Bedrock client\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "\n",
    "# Set MLflow tracking URI\n",
    "tracking_server_arn = \"arn:aws:sagemaker:us-east-1:403678423963:mlflow-tracking-server/tracking-server-48y30z4n7vcief-6bcxt1d5eee7qf-dev\" # Enter ARN\n",
    "mlflow.set_tracking_uri(tracking_server_arn) \n",
    "\n",
    "print(\"‚úÖ AWS Bedrock client initialized\")\n",
    "print(\"‚úÖ MLflow 3.3+ tracking configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T15:37:37.359669Z",
     "iopub.status.busy": "2025-09-08T15:37:37.359395Z",
     "iopub.status.idle": "2025-09-08T15:37:37.776425Z",
     "shell.execute_reply": "2025-09-08T15:37:37.775795Z",
     "shell.execute_reply.started": "2025-09-08T15:37:37.359649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new experiment: bedrock-prompt-management-v3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://amazon-sagemaker-403678423963-us-east-1-7e1d8ee23c03/dzd_5l4vev2ybtbenb/48y30z4n7vcief/dev/data/ml/mlflow/35', creation_time=1757345857684, experiment_id='35', last_update_time=1757345857684, lifecycle_stage='active', name='bedrock-prompt-management-v3', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new experiment\n",
    "experiment_name = \"bedrock-prompt-management-v3\"\n",
    "try:\n",
    "    experiment_id = mlflow.create_experiment(experiment_name)\n",
    "    print(f\"Created new experiment: {experiment_name}\")\n",
    "except mlflow.exceptions.MlflowException:\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    experiment_id = experiment.experiment_id\n",
    "    print(f\"Using existing experiment: {experiment_name}\")\n",
    "\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Register Prompt Template (MLflow 3.3+ API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T15:38:37.003380Z",
     "iopub.status.busy": "2025-09-08T15:38:37.003076Z",
     "iopub.status.idle": "2025-09-08T15:38:38.096559Z",
     "shell.execute_reply": "2025-09-08T15:38:38.095779Z",
     "shell.execute_reply.started": "2025-09-08T15:38:37.003358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering customer support prompt template...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/08 15:38:37 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: customer-support-prompt, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Registered prompt: customer-support-prompt (version 1)\n"
     ]
    }
   ],
   "source": [
    "# Register a prompt template using MLflow 3.3+ API\n",
    "print(\"Registering customer support prompt template...\")\n",
    "\n",
    "try:\n",
    "    # Note: MLflow 3.3+ uses {{variable}} syntax (double curly braces)\n",
    "    prompt_template = \"You are a helpful customer support agent. \"\\\n",
    "                     \"Customer question: {{question}}\\n\"\\\n",
    "                     \"Product context: {{product_info}}\\n\"\\\n",
    "                     \"Please provide a helpful and professional response.\"\n",
    "    \n",
    "    customer_support_prompt = mlflow.register_prompt(\n",
    "        name=\"customer-support-prompt\",\n",
    "        template=prompt_template,\n",
    "        commit_message=\"Initial customer support prompt template\",\n",
    "        tags={\"use_case\": \"customer_support\", \"version\": \"v1\"}\n",
    "    )\n",
    "    print(f\"‚úÖ Registered prompt: {customer_support_prompt.name} (version {customer_support_prompt.version})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Prompt registration error: {e}\")\n",
    "    print(\"This might be expected if the prompt already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use Prompt Template with Bedrock Converse API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T15:47:22.250356Z",
     "iopub.status.busy": "2025-09-08T15:47:22.250063Z",
     "iopub.status.idle": "2025-09-08T15:47:30.838462Z",
     "shell.execute_reply": "2025-09-08T15:47:30.837632Z",
     "shell.execute_reply.started": "2025-09-08T15:47:22.250335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded prompt: customer-support-prompt (version 1)\n",
      "Filled prompt:\n",
      "You are a helpful customer support agent. Customer question: How do I return a defective product?\n",
      "Product context: Electronics with 30-day return policy\n",
      "Please provide a helpful and professional response.\n",
      "\n",
      "Model response:\n",
      "I'd be happy to help you return your defective electronics product. Here's how to process your return:\n",
      "\n",
      "**Return Process:**\n",
      "1. **Check eligibility**: Your product is covered by our 30-day return policy from the date of purchase\n",
      "2. **Gather required items**: \n",
      "   - Original receipt or order confirmation\n",
      "   - The defective product and all original accessories\n",
      "   - Original packaging (if available)\n",
      "\n",
      "**Next Steps:**\n",
      "- **Online orders**: Log into your account and initiate a return request, or contact us directly\n",
      "- **In-store purchases**: Visit any of our store locations with your receipt\n",
      "- **Phone support**: Call our returns department at [phone number] for assistance\n",
      "\n",
      "**What to expect:**\n",
      "- We'll provide a prepaid return shipping label for online returns\n",
      "- Full refund will be processed within 5-7 business days after we receive the item\n",
      "- You can choose a replacement product instead of a refund if preferred\n",
      "\n",
      "**Need immediate help?** If you have your order number ready, I can help you start the return process right now.\n",
      "\n",
      "Is there anything specific about the defect or return process you'd like me to clarify?\n",
      "üèÉ View run basic_customer_support at: https://us-east-1.experiments.sagemaker.aws/#/experiments/35/runs/2da4968a6a3b4446bdf1b6e5ba6e7a54\n",
      "üß™ View experiment at: https://us-east-1.experiments.sagemaker.aws/#/experiments/35\n"
     ]
    }
   ],
   "source": [
    "# Use the prompt template with Bedrock Converse API directly\n",
    "\n",
    "with mlflow.start_run(run_name=\"basic_customer_support\"):\n",
    "    # Load the prompt template using MLflow 3.3+ API\n",
    "    try:\n",
    "        prompt = mlflow.genai.load_prompt(\"customer-support-prompt\", version=1)\n",
    "        print(f\"Loaded prompt: {prompt.name} (version {prompt.version})\")\n",
    "        \n",
    "        # Fill in the template variables using MLflow 3.3+ format method\n",
    "        filled_prompt = prompt.format(\n",
    "            question=\"How do I return a defective product?\",\n",
    "            product_info=\"Electronics with 30-day return policy\"\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading prompt: {e}\")\n",
    "        # Fallback to manual template\n",
    "        filled_prompt = \"You are a helpful customer support agent. \"\\\n",
    "                       \"Customer question: How do I return a defective product?\\n\"\\\n",
    "                       \"Product context: Electronics with 30-day return policy\\n\"\\\n",
    "                       \"Please provide a helpful and professional response.\"\n",
    "    \n",
    "    print(f\"Filled prompt:\\n{filled_prompt}\\n\")\n",
    "    \n",
    "    # Log parameters\n",
    "    model_id = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "    mlflow.log_param(\"model_id\", model_id)\n",
    "    mlflow.log_param(\"prompt_name\", \"customer-support-prompt\")\n",
    "    mlflow.log_param(\"max_tokens\", 1000)\n",
    "    mlflow.log_param(\"temperature\", 0.7)\n",
    "    \n",
    "    # Log the prompt\n",
    "    mlflow.log_text(filled_prompt, \"input_prompt.txt\")\n",
    "    \n",
    "    # Invoke Bedrock model using Converse API directly\n",
    "    try:\n",
    "        response = bedrock_runtime.converse(\n",
    "            modelId=model_id,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"text\": filled_prompt\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": 1000,\n",
    "                \"temperature\": 0.7,\n",
    "                \"topP\": 0.9\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Extract response text\n",
    "        response_text = response['output']['message']['content'][0]['text']\n",
    "        usage = response.get('usage', {})\n",
    "        stop_reason = response['stopReason']\n",
    "        \n",
    "        # Log response and metrics\n",
    "        mlflow.log_text(response_text, \"model_response.txt\")\n",
    "        mlflow.log_metric(\"response_length\", len(response_text))\n",
    "        mlflow.log_metric(\"prompt_length\", len(filled_prompt))\n",
    "        mlflow.log_param(\"stop_reason\", stop_reason)\n",
    "        \n",
    "        # Log token usage if available\n",
    "        if usage:\n",
    "            mlflow.log_metric(\"input_tokens\", usage.get(\"inputTokens\", 0))\n",
    "            mlflow.log_metric(\"output_tokens\", usage.get(\"outputTokens\", 0))\n",
    "            mlflow.log_metric(\"total_tokens\", usage.get(\"totalTokens\", 0))\n",
    "        \n",
    "        print(f\"Model response:\\n{response_text}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error invoking Bedrock model: {e}\"\n",
    "        print(error_msg)\n",
    "        mlflow.log_text(error_msg, \"error_log.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. System Prompt Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T15:48:45.099856Z",
     "iopub.status.busy": "2025-09-08T15:48:45.099577Z",
     "iopub.status.idle": "2025-09-08T15:48:46.074441Z",
     "shell.execute_reply": "2025-09-08T15:48:46.073726Z",
     "shell.execute_reply.started": "2025-09-08T15:48:45.099836Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/08 15:48:45 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: teaching-system-prompt, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Registered system prompt: teaching-system-prompt\n"
     ]
    }
   ],
   "source": [
    "# Register a system prompt template\n",
    "try:\n",
    "    system_prompt_template = mlflow.genai.register_prompt(\n",
    "        name=\"teaching-system-prompt\",\n",
    "        template=\"You are an excellent teacher who explains complex concepts in simple, engaging ways. \"\\\n",
    "                \"Always use analogies and examples that a {{age_group}} can understand.\",\n",
    "        commit_message=\"System prompt for teaching complex topics\",\n",
    "        tags={\"type\": \"system_prompt\", \"domain\": \"education\"}\n",
    "    )\n",
    "    print(f\"‚úÖ Registered system prompt: {system_prompt_template.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"System prompt registration: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T15:50:10.456383Z",
     "iopub.status.busy": "2025-09-08T15:50:10.456100Z",
     "iopub.status.idle": "2025-09-08T15:50:23.131583Z",
     "shell.execute_reply": "2025-09-08T15:50:23.130923Z",
     "shell.execute_reply.started": "2025-09-08T15:50:10.456364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt Response:\n",
      "Sure, I'll explain machine learning in a way that's easy to understand.\n",
      "\n",
      "Imagine you have a really smart student in your class named \"Machine\". Machine is great at following instructions, but not so great at figuring things out on its own initially. However, Machine is an incredibly fast learner.\n",
      "\n",
      "With machine learning, instead of programming Machine with a bunch of rules and instructions like traditional programming, we feed it a ton of examples and data. It's like giving Machine lots and lots of homework assignments to practice on.\n",
      "\n",
      "At first, Machine may not do so well on the assignments because it doesn't know much yet. But as Machine works through more and more examples, it starts picking up on patterns in the data. It learns from its mistakes and gets better and better at the task over time.\n",
      "\n",
      "Eventually, after seeing enough examples, Machine becomes amazingly good at that type of homework. It can then use what it learned from the examples to solve new problems it hasn't seen before - just like a human student would after enough practice.\n",
      "\n",
      "That's the core idea behind machine learning. We're not programming Machine directly with rules. Instead, we give it data as examples to allow it to learn and figure out its own patterns and rules. The more data it learns from, the smarter it gets.\n",
      "üèÉ View run system_prompt_example at: https://us-east-1.experiments.sagemaker.aws/#/experiments/35/runs/a30647e824a14118b432555cb0435f20\n",
      "üß™ View experiment at: https://us-east-1.experiments.sagemaker.aws/#/experiments/35\n"
     ]
    }
   ],
   "source": [
    "# Example with system prompt\n",
    "with mlflow.start_run(run_name=\"system_prompt_example\"):\n",
    "    user_message = \"Explain machine learning in simple terms.\"\n",
    "    \n",
    "    # Load and format system prompt\n",
    "    try:\n",
    "        system_prompt_template = mlflow.genai.load_prompt(\"teaching-system-prompt\", version=1)\n",
    "        system_prompt = system_prompt_template.format(age_group=\"12-year-old\")\n",
    "    except Exception as e:\n",
    "        print(f\"Using fallback system prompt: {e}\")\n",
    "        system_prompt = \"You are an excellent teacher who explains complex concepts simply.\"\n",
    "    \n",
    "    # Log parameters\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "    mlflow.log_param(\"model_id\", model_id)\n",
    "    mlflow.log_param(\"has_system_prompt\", True)\n",
    "    mlflow.log_param(\"system_prompt_name\", \"teaching-system-prompt\")\n",
    "    \n",
    "    # Log prompts\n",
    "    mlflow.log_text(system_prompt, \"system_prompt.txt\")\n",
    "    mlflow.log_text(user_message, \"user_message.txt\")\n",
    "    \n",
    "    # Get response using Converse API with system prompt\n",
    "    try:\n",
    "        response = bedrock_runtime.converse(\n",
    "            modelId=model_id,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"text\": user_message\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            system=[\n",
    "                {\n",
    "                    \"text\": system_prompt\n",
    "                }\n",
    "            ],\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": 1500,\n",
    "                \"temperature\": 0.7,\n",
    "                \"topP\": 0.9\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Extract response details\n",
    "        response_text = response['output']['message']['content'][0]['text']\n",
    "        usage = response.get('usage', {})\n",
    "        \n",
    "        # Log response and metrics\n",
    "        mlflow.log_text(response_text, \"model_response.txt\")\n",
    "        mlflow.log_metric(\"response_length\", len(response_text))\n",
    "        \n",
    "        if usage:\n",
    "            mlflow.log_metric(\"input_tokens\", usage.get(\"inputTokens\", 0))\n",
    "            mlflow.log_metric(\"output_tokens\", usage.get(\"outputTokens\", 0))\n",
    "            mlflow.log_metric(\"total_tokens\", usage.get(\"totalTokens\", 0))\n",
    "        \n",
    "        print(f\"System Prompt Response:\\n{response_text}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error with system prompt: {e}\"\n",
    "        print(error_msg)\n",
    "        mlflow.log_text(error_msg, \"error_log.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Search and Manage Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T15:54:08.121981Z",
     "iopub.status.busy": "2025-09-08T15:54:08.121704Z",
     "iopub.status.idle": "2025-09-08T15:54:08.360446Z",
     "shell.execute_reply": "2025-09-08T15:54:08.359676Z",
     "shell.execute_reply.started": "2025-09-08T15:54:08.121961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for registered prompts...\n",
      "Found 3 registered prompts:\n",
      "- customer-support-prompt\n",
      "  Description: Initial customer support prompt template\n",
      "  Tags: {'use_case': 'customer_support', 'version': 'v1'}\n",
      "  Latest version: 1\n",
      "  Template preview: You are a helpful customer support agent. Customer question: {{question}}\n",
      "Product context: {{product...\n",
      "\n",
      "- summarization-prompt\n",
      "  Description: Initial version of summarization prompt\n",
      "  Latest version: 1\n",
      "  Template preview: \n",
      "Summarize the provided content in {{ num_sentences }} sentences.\n",
      "Content: {{ content }}\n",
      "...\n",
      "\n",
      "- teaching-system-prompt\n",
      "  Description: System prompt for teaching complex topics\n",
      "  Tags: {'type': 'system_prompt', 'domain': 'education'}\n",
      "  Latest version: 1\n",
      "  Template preview: You are an excellent teacher who explains complex concepts in simple, engaging ways. Always use anal...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search for registered prompts\n",
    "print(\"Searching for registered prompts...\")\n",
    "try:\n",
    "    prompts = mlflow.genai.search_prompts()\n",
    "    print(f\"Found {len(prompts)} registered prompts:\")\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"- {prompt.name}\")\n",
    "        if hasattr(prompt, 'description') and prompt.description:\n",
    "            print(f\"  Description: {prompt.description}\")\n",
    "        if hasattr(prompt, 'tags') and prompt.tags:\n",
    "            print(f\"  Tags: {prompt.tags}\")\n",
    "        \n",
    "        # To get version info, we need to load the prompt\n",
    "        try:\n",
    "            loaded_prompt = mlflow.genai.load_prompt(prompt.name, version=1)\n",
    "            print(f\"  Latest version: {loaded_prompt.version}\")\n",
    "            print(f\"  Template preview: {loaded_prompt.template[:100]}...\")\n",
    "        except Exception as load_error:\n",
    "            print(f\"  Could not load version info: {load_error}\")\n",
    "        print()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error searching prompts: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T15:54:55.399751Z",
     "iopub.status.busy": "2025-09-08T15:54:55.399491Z",
     "iopub.status.idle": "2025-09-08T15:54:55.563538Z",
     "shell.execute_reply": "2025-09-08T15:54:55.562864Z",
     "shell.execute_reply.started": "2025-09-08T15:54:55.399732Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ID: 35\n",
      "Experiment Name: bedrock-prompt-management-v3\n",
      "\n",
      "Total runs in experiment: 6\n",
      "\n",
      "Run Summary:\n",
      "- system_prompt_example: FINISHED\n",
      "  Total tokens: 318\n",
      "  Response length: 1308 chars\n",
      "\n",
      "- basic_customer_support: FINISHED\n",
      "  Total tokens: 303\n",
      "  Response length: 1116 chars\n",
      "\n",
      "- basic_customer_support: FINISHED\n",
      "  Total tokens: 318\n",
      "  Response length: 1136 chars\n",
      "\n",
      "- basic_customer_support: FINISHED\n",
      "  Total tokens: 463\n",
      "  Response length: 1969 chars\n",
      "\n",
      "- basic_customer_support: FINISHED\n",
      "  Total tokens: 297\n",
      "  Response length: 1053 chars\n",
      "\n",
      "- basic_customer_support: FINISHED\n",
      "  Total tokens: 361\n",
      "  Response length: 1450 chars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get experiment information\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "print(f\"Experiment ID: {experiment.experiment_id}\")\n",
    "print(f\"Experiment Name: {experiment.name}\")\n",
    "\n",
    "# Get all runs from the experiment\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "print(f\"\\nTotal runs in experiment: {len(runs)}\")\n",
    "\n",
    "# Display summary of runs\n",
    "if len(runs) > 0:\n",
    "    print(\"\\nRun Summary:\")\n",
    "    for idx, run in runs.iterrows():\n",
    "        print(f\"- {run['tags.mlflow.runName']}: {run['status']}\")\n",
    "        if 'metrics.total_tokens' in run and run['metrics.total_tokens'] is not None:\n",
    "            print(f\"  Total tokens: {int(run['metrics.total_tokens'])}\")\n",
    "        if 'metrics.response_length' in run and run['metrics.response_length'] is not None:\n",
    "            print(f\"  Response length: {int(run['metrics.response_length'])} chars\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T16:19:03.820463Z",
     "iopub.status.busy": "2025-09-08T16:19:03.820190Z",
     "iopub.status.idle": "2025-09-08T16:19:04.692968Z",
     "shell.execute_reply": "2025-09-08T16:19:04.692294Z",
     "shell.execute_reply.started": "2025-09-08T16:19:03.820443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating version 2 of customer support prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/08 16:19:04 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: customer-support-prompt, version 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created version 2: customer-support-prompt (version 2)\n",
      "Commit message: Enhanced customer support prompt with structured approach and empathy\n"
     ]
    }
   ],
   "source": [
    "# Create version 2 of the customer support prompt with improvements\n",
    "print(\"Creating version 2 of customer support prompt...\")\n",
    "\n",
    "try:\n",
    "    # Enhanced prompt template with more structure and empathy\n",
    "    enhanced_prompt_template = \"\"\"You are an expert customer support specialist with 10+ years of experience.\n",
    "    \n",
    "Customer Information:\n",
    "- Question: {{question}}\n",
    "- Product Context: {{product_info}}\n",
    "- Customer Tier: {{customer_tier}}\n",
    "- Urgency Level: {{urgency}}\n",
    "\n",
    "Instructions:\n",
    "1. Acknowledge the customer's concern with empathy\n",
    "2. Provide a clear, step-by-step solution\n",
    "3. Offer additional resources or escalation if needed\n",
    "4. End with a professional closing and next steps\n",
    "\n",
    "Please provide a comprehensive, helpful response.\"\"\"\n",
    "    \n",
    "    # Register the enhanced version (this will create version 2)\n",
    "    customer_support_v2 = mlflow.genai.register_prompt(\n",
    "        name=\"customer-support-prompt\",  # Same name creates new version\n",
    "        template=enhanced_prompt_template,\n",
    "        commit_message=\"Enhanced customer support prompt with structured approach and empathy\",\n",
    "        tags={\"use_case\": \"customer_support\", \"version\": \"v2\", \"enhancement\": \"structured_empathetic\"}\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Created version 2: {customer_support_v2.name} (version {customer_support_v2.version})\")\n",
    "    print(f\"Commit message: {customer_support_v2.commit_message}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating version 2: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T16:20:35.673380Z",
     "iopub.status.busy": "2025-09-08T16:20:35.673086Z",
     "iopub.status.idle": "2025-09-08T16:20:35.819519Z",
     "shell.execute_reply": "2025-09-08T16:20:35.818559Z",
     "shell.execute_reply.started": "2025-09-08T16:20:35.673360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing prompt versions...\n",
      "\n",
      "üìã VERSION 1 (Original):\n",
      "Template length: 159 characters\n",
      "Template preview: You are a helpful customer support agent. Customer question: {{question}}\n",
      "Product context: {{product_info}}\n",
      "Please provide a helpful and professional ...\n",
      "\n",
      "üìã VERSION 2 (Enhanced):\n",
      "Template length: 490 characters\n",
      "Template preview: You are an expert customer support specialist with 10+ years of experience.\n",
      "    \n",
      "Customer Information:\n",
      "- Question: {{question}}\n",
      "- Product Context: {{p...\n",
      "\n",
      "üìä COMPARISON:\n",
      "Length difference: 331 characters\n",
      "V1 Variables: 2 variables\n",
      "V2 Variables: 4 variables\n"
     ]
    }
   ],
   "source": [
    "# Load and compare both versions of the prompt\n",
    "print(\"Comparing prompt versions...\\n\")\n",
    "\n",
    "try:\n",
    "    # Load version 1\n",
    "    prompt_v1 = mlflow.genai.load_prompt(\"customer-support-prompt\", version=1)\n",
    "    print(f\"üìã VERSION 1 (Original):\")\n",
    "    print(f\"Template length: {len(prompt_v1.template)} characters\")\n",
    "    print(f\"Template preview: {prompt_v1.template[:150]}...\\n\")\n",
    "    \n",
    "    # Load version 2\n",
    "    prompt_v2 = mlflow.genai.load_prompt(\"customer-support-prompt\", version=2)\n",
    "    print(f\"üìã VERSION 2 (Enhanced):\")\n",
    "    print(f\"Template length: {len(prompt_v2.template)} characters\")\n",
    "    print(f\"Template preview: {prompt_v2.template[:150]}...\\n\")\n",
    "    \n",
    "    # Show the difference in structure\n",
    "    print(f\"üìä COMPARISON:\")\n",
    "    print(f\"Length difference: {len(prompt_v2.template) - len(prompt_v1.template)} characters\")\n",
    "    print(f\"V1 Variables: {len(prompt_v1.template.split('{{'))-1} variables\")\n",
    "    print(f\"V2 Variables: {len(prompt_v2.template.split('{{'))-1} variables\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error comparing versions: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T16:22:41.298441Z",
     "iopub.status.busy": "2025-09-08T16:22:41.298137Z",
     "iopub.status.idle": "2025-09-08T16:23:04.953162Z",
     "shell.execute_reply": "2025-09-08T16:23:04.952531Z",
     "shell.execute_reply.started": "2025-09-08T16:22:41.298421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ A/B Testing Prompt Versions...\n",
      "\n",
      "Test Scenario: My laptop arrived damaged and I need a replacement urgently for work\n",
      "\n",
      "‚úÖ VERSION 1 COMPLETED:\n",
      "   Response: 1606 chars, 261 words\n",
      "   Time: 9.99s, Tokens: 398\n",
      "   Preview: Dear Valued Customer,\n",
      "\n",
      "Thank you for reaching out to us regarding the issue with your recently purchased Dell XPS 13 laptop. I understand how frustrating it can be to receive a damaged product, especi...\n",
      "\n",
      "üèÉ View run customer_support_v1_ab_test at: https://us-east-1.experiments.sagemaker.aws/#/experiments/35/runs/416b2d98f3864d4086ab393dcdd6d8c8\n",
      "üß™ View experiment at: https://us-east-1.experiments.sagemaker.aws/#/experiments/35\n",
      "‚úÖ VERSION 2 COMPLETED:\n",
      "   Response: 1443 chars, 241 words\n",
      "   Time: 8.56s, Tokens: 453\n",
      "   Preview: Dear Valued Customer,\n",
      "\n",
      "I'm very sorry to hear that your new Dell XPS 13 laptop arrived damaged. I understand how frustrating and inconvenient this must be, especially given your urgent need for a work...\n",
      "\n",
      "üèÉ View run customer_support_v2_ab_test at: https://us-east-1.experiments.sagemaker.aws/#/experiments/35/runs/3c69b38c39e64372a6ca3d91c03ea893\n",
      "üß™ View experiment at: https://us-east-1.experiments.sagemaker.aws/#/experiments/35\n",
      "üìä A/B TEST COMPARISON:\n",
      "Metric               V1           V2           Difference     \n",
      "------------------------------------------------------------\n",
      "Response Length      1606.00      1443.00      -163.00 (-10.1%)\n",
      "Word Count           261.00       241.00       -20.00 (-7.7%)\n",
      "Total Tokens         398.00       453.00       +55.00 (+13.8%)\n",
      "Response Time        9.99         8.56         -1.44 (-14.4%)\n",
      "\n",
      "üí° Key Insights:\n",
      "‚Ä¢ V2 uses more tokens (higher cost, potentially better quality)\n",
      "‚Ä¢ Compare response quality manually to determine the winner\n"
     ]
    }
   ],
   "source": [
    "# A/B test both prompt versions with the same input\n",
    "import time\n",
    "\n",
    "# Test scenario\n",
    "test_scenario = {\n",
    "    \"question\": \"My laptop arrived damaged and I need a replacement urgently for work\",\n",
    "    \"product_info\": \"Dell XPS 13 laptop, Premium warranty, ordered 3 days ago\",\n",
    "    \"customer_tier\": \"Premium\",\n",
    "    \"urgency\": \"High\"\n",
    "}\n",
    "\n",
    "print(\"üß™ A/B Testing Prompt Versions...\\n\")\n",
    "print(f\"Test Scenario: {test_scenario['question']}\\n\")\n",
    "\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "results = {}\n",
    "\n",
    "# Test both versions\n",
    "for version in [1, 2]:\n",
    "    with mlflow.start_run(run_name=f\"customer_support_v{version}_ab_test\"):\n",
    "        try:\n",
    "            # Load prompt version\n",
    "            prompt = mlflow.genai.load_prompt(\"customer-support-prompt\", version=version)\n",
    "            \n",
    "            # Format prompt (V1 uses fewer variables)\n",
    "            if version == 1:\n",
    "                filled_prompt = prompt.format(\n",
    "                    question=test_scenario[\"question\"],\n",
    "                    product_info=test_scenario[\"product_info\"]\n",
    "                )\n",
    "            else:\n",
    "                filled_prompt = prompt.format(**test_scenario)\n",
    "            \n",
    "            # Log parameters\n",
    "            mlflow.log_param(\"prompt_version\", f\"v{version}\")\n",
    "            mlflow.log_param(\"model_id\", model_id)\n",
    "            mlflow.log_param(\"test_type\", \"ab_test\")\n",
    "            mlflow.log_text(filled_prompt, f\"prompt_v{version}.txt\")\n",
    "            \n",
    "            # Call Bedrock\n",
    "            start_time = time.time()\n",
    "            response = bedrock_runtime.converse(\n",
    "                modelId=model_id,\n",
    "                messages=[{\"role\": \"user\", \"content\": [{\"text\": filled_prompt}]}],\n",
    "                inferenceConfig={\"maxTokens\": 1500, \"temperature\": 0.7, \"topP\": 0.9}\n",
    "            )\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            # Extract results\n",
    "            response_text = response['output']['message']['content'][0]['text']\n",
    "            usage = response.get('usage', {})\n",
    "            \n",
    "            # Log metrics\n",
    "            mlflow.log_text(response_text, f\"response_v{version}.txt\")\n",
    "            mlflow.log_metric(\"response_time_seconds\", response_time)\n",
    "            mlflow.log_metric(\"response_length\", len(response_text))\n",
    "            mlflow.log_metric(\"response_word_count\", len(response_text.split()))\n",
    "            mlflow.log_metric(\"prompt_length\", len(filled_prompt))\n",
    "            \n",
    "            if usage:\n",
    "                mlflow.log_metric(\"total_tokens\", usage.get(\"totalTokens\", 0))\n",
    "                mlflow.log_metric(\"input_tokens\", usage.get(\"inputTokens\", 0))\n",
    "                mlflow.log_metric(\"output_tokens\", usage.get(\"outputTokens\", 0))\n",
    "            \n",
    "            # Store results for comparison\n",
    "            results[f\"v{version}\"] = {\n",
    "                \"response_text\": response_text,\n",
    "                \"response_time\": response_time,\n",
    "                \"response_length\": len(response_text),\n",
    "                \"word_count\": len(response_text.split()),\n",
    "                \"total_tokens\": usage.get(\"totalTokens\", 0)\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ VERSION {version} COMPLETED:\")\n",
    "            print(f\"   Response: {len(response_text)} chars, {len(response_text.split())} words\")\n",
    "            print(f\"   Time: {response_time:.2f}s, Tokens: {usage.get('totalTokens', 0)}\")\n",
    "            print(f\"   Preview: {response_text[:200]}...\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error testing V{version}: {e}\")\n",
    "            mlflow.log_text(str(e), f\"error_v{version}.txt\")\n",
    "\n",
    "# Compare results\n",
    "if len(results) == 2:\n",
    "    print(\"üìä A/B TEST COMPARISON:\")\n",
    "    print(f\"{'Metric':<20} {'V1':<12} {'V2':<12} {'Difference':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    v1, v2 = results[\"v1\"], results[\"v2\"]\n",
    "    \n",
    "    metrics = [\n",
    "        (\"Response Length\", \"response_length\"),\n",
    "        (\"Word Count\", \"word_count\"),\n",
    "        (\"Total Tokens\", \"total_tokens\"),\n",
    "        (\"Response Time\", \"response_time\")\n",
    "    ]\n",
    "    \n",
    "    for name, key in metrics:\n",
    "        v1_val = v1[key]\n",
    "        v2_val = v2[key]\n",
    "        diff = v2_val - v1_val\n",
    "        pct = (diff / v1_val * 100) if v1_val > 0 else 0\n",
    "        print(f\"{name:<20} {v1_val:<12.2f} {v2_val:<12.2f} {diff:+.2f} ({pct:+.1f}%)\")\n",
    "    \n",
    "    print(\"\\nüí° Key Insights:\")\n",
    "    if v2[\"response_length\"] > v1[\"response_length\"]:\n",
    "        print(\"‚Ä¢ V2 generates more detailed responses\")\n",
    "    if v2[\"total_tokens\"] > v1[\"total_tokens\"]:\n",
    "        print(\"‚Ä¢ V2 uses more tokens (higher cost, potentially better quality)\")\n",
    "    print(\"‚Ä¢ Compare response quality manually to determine the winner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T17:18:45.900971Z",
     "iopub.status.busy": "2025-09-08T17:18:45.900690Z",
     "iopub.status.idle": "2025-09-08T17:18:45.906261Z",
     "shell.execute_reply": "2025-09-08T17:18:45.905335Z",
     "shell.execute_reply.started": "2025-09-08T17:18:45.900949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Setting up simple model evaluation...\n",
      "‚úÖ Configured 2 models for evaluation\n",
      "‚úÖ Test prompt ready (220 characters)\n"
     ]
    }
   ],
   "source": [
    "# Simple model evaluation setup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üî¨ Setting up simple model evaluation...\")\n",
    "\n",
    "# Models to compare\n",
    "models_to_compare = [\n",
    "    {\n",
    "        \"name\": \"Claude 3 Sonnet\",\n",
    "        \"model_id\": \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        \"generation\": \"3\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Claude 4 Sonnet\",\n",
    "        \"model_id\": \"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "        \"generation\": \"4\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Simple test prompt\n",
    "test_prompt = \"\"\"You are a helpful customer support agent.\n",
    "\n",
    "Customer question: I'm having trouble with my subscription renewal. It keeps failing when I try to update my payment method.\n",
    "\n",
    "Please provide a helpful solution with clear steps.\"\"\"\n",
    "\n",
    "print(f\"‚úÖ Configured {len(models_to_compare)} models for evaluation\")\n",
    "print(f\"‚úÖ Test prompt ready ({len(test_prompt)} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T17:19:33.450438Z",
     "iopub.status.busy": "2025-09-08T17:19:33.450129Z",
     "iopub.status.idle": "2025-09-08T17:19:55.225749Z",
     "shell.execute_reply": "2025-09-08T17:19:55.224996Z",
     "shell.execute_reply.started": "2025-09-08T17:19:33.450416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Evaluating models...\n",
      "\n",
      "Testing Claude 3 Sonnet...\n",
      "  ‚úÖ Response: 303 words, 9.53s, 439 tokens\n",
      "  Preview: I'm sorry to hear you're having trouble with your subscription renewal. Let me provide some steps to...\n",
      "\n",
      "üèÉ View run eval_claude_3_sonnet at: https://us-east-1.experiments.sagemaker.aws/#/experiments/35/runs/0d69055b3b4149589d21ad421b976f41\n",
      "üß™ View experiment at: https://us-east-1.experiments.sagemaker.aws/#/experiments/35\n",
      "Testing Claude 4 Sonnet...\n",
      "  ‚úÖ Response: 213 words, 6.96s, 356 tokens\n",
      "  Preview: I'd be happy to help you resolve this payment method issue. Here are some steps to troubleshoot and ...\n",
      "\n",
      "üèÉ View run eval_claude_4_sonnet at: https://us-east-1.experiments.sagemaker.aws/#/experiments/35/runs/6e5d13a43f6f45d8964237551e5834b8\n",
      "üß™ View experiment at: https://us-east-1.experiments.sagemaker.aws/#/experiments/35\n",
      "‚úÖ Evaluation completed with 2 results\n"
     ]
    }
   ],
   "source": [
    "# Evaluate both models with the same prompt\n",
    "print(\"ü§ñ Evaluating models...\\n\")\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for model in models_to_compare:\n",
    "    print(f\"Testing {model['name']}...\")\n",
    "    \n",
    "    # Create MLflow run for this model evaluation\n",
    "    with mlflow.start_run(run_name=f\"eval_{model['name'].replace(' ', '_').lower()}\"):\n",
    "        try:\n",
    "            # Log model information\n",
    "            mlflow.log_param(\"model_name\", model[\"name\"])\n",
    "            mlflow.log_param(\"model_id\", model[\"model_id\"])\n",
    "            mlflow.log_param(\"model_generation\", model[\"generation\"])\n",
    "            mlflow.log_param(\"evaluation_type\", \"simple_model_comparison\")\n",
    "            \n",
    "            # Log the test prompt\n",
    "            mlflow.log_text(test_prompt, \"test_prompt.txt\")\n",
    "            \n",
    "            # Call Bedrock with timing\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = bedrock_runtime.converse(\n",
    "                modelId=model[\"model_id\"],\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"text\": test_prompt\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                inferenceConfig={\n",
    "                    \"maxTokens\": 1000,\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"topP\": 0.9\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            # Extract response details\n",
    "            response_text = response['output']['message']['content'][0]['text']\n",
    "            usage = response.get('usage', {})\n",
    "            stop_reason = response['stopReason']\n",
    "            \n",
    "            # Calculate simple metrics\n",
    "            word_count = len(response_text.split())\n",
    "            char_count = len(response_text)\n",
    "            sentence_count = len([s for s in response_text.split('.') if s.strip()])\n",
    "            \n",
    "            # Log metrics to MLflow\n",
    "            mlflow.log_metric(\"response_time_seconds\", response_time)\n",
    "            mlflow.log_metric(\"response_word_count\", word_count)\n",
    "            mlflow.log_metric(\"response_char_count\", char_count)\n",
    "            mlflow.log_metric(\"sentence_count\", sentence_count)\n",
    "            mlflow.log_param(\"stop_reason\", stop_reason)\n",
    "            \n",
    "            # Log token usage if available\n",
    "            if usage:\n",
    "                input_tokens = usage.get(\"inputTokens\", 0)\n",
    "                output_tokens = usage.get(\"outputTokens\", 0)\n",
    "                total_tokens = usage.get(\"totalTokens\", 0)\n",
    "                \n",
    "                mlflow.log_metric(\"input_tokens\", input_tokens)\n",
    "                mlflow.log_metric(\"output_tokens\", output_tokens)\n",
    "                mlflow.log_metric(\"total_tokens\", total_tokens)\n",
    "                \n",
    "                # Simple cost estimate (approximate)\n",
    "                estimated_cost = (input_tokens * 0.003 + output_tokens * 0.015) / 1000\n",
    "                mlflow.log_metric(\"estimated_cost_usd\", estimated_cost)\n",
    "            \n",
    "            # Log the full response\n",
    "            mlflow.log_text(response_text, \"model_response.txt\")\n",
    "            \n",
    "            # Store results for comparison\n",
    "            result = {\n",
    "                \"model_name\": model[\"name\"],\n",
    "                \"generation\": model[\"generation\"],\n",
    "                \"response_time\": response_time,\n",
    "                \"word_count\": word_count,\n",
    "                \"char_count\": char_count,\n",
    "                \"total_tokens\": total_tokens if usage else 0,\n",
    "                \"estimated_cost\": estimated_cost if usage else 0,\n",
    "                \"response_preview\": response_text[:150] + \"...\"\n",
    "            }\n",
    "            evaluation_results.append(result)\n",
    "            \n",
    "            print(f\"  ‚úÖ Response: {word_count} words, {response_time:.2f}s, {total_tokens if usage else 0} tokens\")\n",
    "            print(f\"  Preview: {response_text[:100]}...\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error evaluating {model['name']}: {e}\"\n",
    "            print(f\"  ‚ùå {error_msg}\")\n",
    "            mlflow.log_text(error_msg, \"evaluation_error.txt\")\n",
    "            \n",
    "            # Log error result\n",
    "            result = {\n",
    "                \"model_name\": model[\"name\"],\n",
    "                \"generation\": model[\"generation\"],\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            evaluation_results.append(result)\n",
    "\n",
    "print(f\"‚úÖ Evaluation completed with {len(evaluation_results)} results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

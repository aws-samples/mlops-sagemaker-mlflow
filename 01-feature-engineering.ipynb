{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Player Churn Dataset\n",
    "\n",
    "This notebook performs feature engineering on the player_churn.csv dataset by selecting specific columns for further analysis. Feature engineering is a critical step in the machine learning pipeline that helps improve model performance by selecting the most relevant features.\n",
    "\n",
    "In this notebook, we'll focus on:\n",
    "1. Loading the player churn dataset\n",
    "2. Selecting the most important features based on domain knowledge\n",
    "3. Exploring the selected features\n",
    "4. Saving the processed dataset for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn \"pandas>=2.0.0\" s3fs==0.4.2 sagemaker xgboost mlflow==2.13.2 sagemaker-mlflow==0.1.0 seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "First, we'll import the necessary libraries for data manipulation, analysis, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import resample\n",
    "import boto3\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "We'll load the player_churn.csv file which contains player behavior data and churn information. This dataset includes various metrics about player sessions, engagement patterns, and whether they churned (stopped playing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/player_churn.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "In this step, we'll select only the most relevant features for our churn prediction model. These features were likely identified through domain expertise or previous analysis as being the most predictive of player churn.\n",
    "\n",
    "The selected features include:\n",
    "- Player identification features (`player_id`, `cohort_id`, `player_type`)\n",
    "- Temporal features (`cohort_day_of_week`)\n",
    "- Engagement metrics (`player_lifetime`, `session_count`)\n",
    "- Session timing patterns (various time-of-day metrics)\n",
    "- Target variable (`player_churn`)\n",
    "\n",
    "By focusing on these specific features, we can build a more efficient and interpretable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to keep\n",
    "cols = ['cohort_day_of_week',\n",
    "       'begin_session_time_of_day_std_last_week_1',\n",
    "       'player_lifetime',\n",
    "       'begin_session_time_of_day_mean_last_day_1',\n",
    "       'end_session_time_of_day_mean_last_week_1',\n",
    "       'begin_session_time_of_day_mean_last_week_1',\n",
    "       'cohort_id',\n",
    "       'player_type',\n",
    "       'begin_session_time_of_day_std_last_day_1',\n",
    "       'end_session_time_of_day_mean_last_day_1',\n",
    "       'begin_session_time_of_day_mean_last_day_2',\n",
    "       'end_session_time_of_day_std_last_week_1',\n",
    "       'end_session_time_of_day_std_last_day_1',\n",
    "       'session_count',\n",
    "       'end_session_time_of_day_mean_last_day_3',\n",
    "       'begin_session_time_of_day_mean_last_day_3',\n",
    "       'end_session_time_of_day_mean_last_day_2',\n",
    "       'player_churn',\n",
    "       'player_id']\n",
    "\n",
    "# Check if all columns exist in the dataset\n",
    "missing_cols = [col for col in cols if col not in df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"Warning: The following columns are not in the dataset: {missing_cols}\")\n",
    "    # Keep only columns that exist in the dataset\n",
    "    cols = [col for col in cols if col in df.columns]\n",
    "\n",
    "# Select only the specified columns\n",
    "df_selected = df[cols]\n",
    "\n",
    "# Display the resulting dataframe\n",
    "print(f\"Selected dataset shape: {df_selected.shape}\")\n",
    "df_selected.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Values\n",
    "\n",
    "For certain time-of-day standard deviation features, we'll fill missing values with 0. This is appropriate for these features as a missing value likely indicates no variation in the session times (e.g., only one session or consistent session times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to fill with 0\n",
    "fill_zero_cols = [\n",
    "    'begin_session_time_of_day_std_last_week_1',\n",
    "    'begin_session_time_of_day_std_last_day_1',\n",
    "    'end_session_time_of_day_std_last_week_1',\n",
    "    'end_session_time_of_day_std_last_day_1'\n",
    "]\n",
    "\n",
    "# Fill missing values with 0 for specified columns\n",
    "for col in fill_zero_cols:\n",
    "    if col in df_selected.columns:\n",
    "        # Count missing values before filling\n",
    "        missing_count = df_selected[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            print(f\"Filling {missing_count} missing values with 0 in column: {col}\")\n",
    "            df_selected.loc[:, col] = df_selected[col].fillna(0)\n",
    "\n",
    "# Verify the missing values were filled\n",
    "missing_after = {col: df_selected[col].isnull().sum() for col in fill_zero_cols if col in df_selected.columns}\n",
    "print(\"\\nRemaining missing values after filling:\")\n",
    "for col, count in missing_after.items():\n",
    "    print(f\"{col}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "We'll apply one-hot encoding to categorical variables to convert them into a format that can be provided to machine learning algorithms. This process creates binary columns for each category in the original categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in categorical columns before encoding\n",
    "if 'player_type' in df_selected.columns:\n",
    "    print(f\"Unique values in player_type: {df_selected['player_type'].nunique()}\")\n",
    "    print(df_selected['player_type'].value_counts())\n",
    "    \n",
    "if 'cohort_id' in df_selected.columns:\n",
    "    print(f\"\\nUnique values in cohort_id: {df_selected['cohort_id'].nunique()}\")\n",
    "    print(df_selected['cohort_id'].value_counts().head())  # Show only top values if many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding\n",
    "# Create dummy variables for player_type and cohort_id\n",
    "cols_to_encode = ['player_type', 'cohort_id']\n",
    "encoded_cols = [col for col in cols_to_encode if col in df_selected.columns]\n",
    "\n",
    "if encoded_cols:\n",
    "    # Get one-hot encoding\n",
    "    df_encoded = pd.get_dummies(df_selected, columns=encoded_cols, prefix=encoded_cols, dtype=int)\n",
    "    \n",
    "    # Display information about the encoded dataset\n",
    "    print(f\"Shape before encoding: {df_selected.shape}\")\n",
    "    print(f\"Shape after encoding: {df_encoded.shape}\")\n",
    "    print(f\"New columns added: {df_encoded.shape[1] - df_selected.shape[1]}\")\n",
    "    \n",
    "    # Update our working dataframe\n",
    "    df_selected = df_encoded\n",
    "    \n",
    "    # Show a sample of the encoded columns\n",
    "    encoded_column_names = [col for col in df_selected.columns if any(col.startswith(prefix + '_') for prefix in encoded_cols)]\n",
    "    print(\"\\nSample of encoded columns:\")\n",
    "    print(encoded_column_names[:10])  # Show first 10 encoded columns\n",
    "    \n",
    "    # Display the first few rows of the encoded dataframe\n",
    "    df_selected.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration After Feature Selection\n",
    "\n",
    "Now that we've selected our features, handled missing values, and encoded categorical variables, let's explore the dataset to better understand the data we'll be working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Analysis\n",
    "\n",
    "Let's check if there are any remaining missing values in our selected features. Missing values can significantly impact model performance and need to be handled appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df_selected.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Summary\n",
    "\n",
    "Let's examine the basic statistics of our numerical features to understand their distributions, ranges, and potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of numerical columns\n",
    "df_selected.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable Analysis\n",
    "\n",
    "Understanding the distribution of our target variable (player_churn) is crucial for model development. An imbalanced distribution might require special handling techniques during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target variable\n",
    "if 'player_churn' in df_selected.columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(x='player_churn', data=df_selected)\n",
    "    plt.title('Distribution of Player Churn')\n",
    "    plt.xlabel('Player Churn (0 = No, 1 = Yes)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate churn rate\n",
    "    churn_rate = df_selected['player_churn'].mean() * 100\n",
    "    print(f\"Churn rate: {churn_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance the Dataset\n",
    "\n",
    "To improve model performance, we'll balance the dataset using random oversampling. This technique involves randomly duplicating samples from the minority class (churned players) to achieve a 1:1 ratio between the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate majority and minority classes\n",
    "df_majority = df_selected[df_selected['player_churn'] == 0]\n",
    "df_minority = df_selected[df_selected['player_churn'] == 1]\n",
    "\n",
    "print(f\"Before oversampling:\\n\"\n",
    "      f\"Number of non-churned players (majority): {len(df_majority)}\\n\"\n",
    "      f\"Number of churned players (minority): {len(df_minority)}\")\n",
    "\n",
    "# Oversample minority class\n",
    "df_minority_oversampled = resample(df_minority, \n",
    "                                   replace=True,     # sample with replacement\n",
    "                                   n_samples=len(df_majority),    # match majority class\n",
    "                                   random_state=42)  # reproducible results\n",
    "\n",
    "# Combine majority class with oversampled minority class\n",
    "df_balanced = pd.concat([df_majority, df_minority_oversampled])\n",
    "\n",
    "# Display new class distribution\n",
    "print(f\"\\nAfter oversampling:\\n\"\n",
    "      f\"Number of non-churned players: {len(df_balanced[df_balanced['player_churn'] == 0])}\\n\"\n",
    "      f\"Number of churned players: {len(df_balanced[df_balanced['player_churn'] == 1])}\")\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Visualize the balanced distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='player_churn', data=df_balanced)\n",
    "plt.title('Distribution of Player Churn After Balancing')\n",
    "plt.xlabel('Player Churn (0 = No, 1 = Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Data Types\n",
    "\n",
    "Some machine learning algorithms require specific data types. Here we'll convert the target variable from boolean to long integer format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current data type of player_churn\n",
    "print(f\"Current data type of player_churn: {df_balanced['player_churn'].dtype}\")\n",
    "\n",
    "# Convert player_churn from boolean to long (int64)\n",
    "df_balanced['player_churn'] = df_balanced['player_churn'].astype('int64')\n",
    "\n",
    "# Verify the conversion\n",
    "print(f\"New data type of player_churn: {df_balanced['player_churn'].dtype}\")\n",
    "\n",
    "# Display a sample of the data to confirm\n",
    "print(\"\\nSample values after conversion:\")\n",
    "print(df_balanced['player_churn'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to SageMaker Feature Store\n",
    "\n",
    "Now we'll save our processed dataset to Amazon SageMaker Feature Store for use in machine learning workflows. Feature Store provides a centralized repository for features, making it easier to share and reuse features across teams and projects.\n",
    "\n",
    "### What is SageMaker Feature Store?\n",
    "\n",
    "Amazon SageMaker Feature Store is a purpose-built repository where you can store and access features so it's much easier to name, organize, and reuse them across teams. Key benefits include:\n",
    "\n",
    "- **Feature Reuse**: Store features once and reuse them for multiple models\n",
    "- **Consistency**: Ensure consistent feature transformations between training and inference\n",
    "- **Discoverability**: Make features discoverable and shareable across your organization\n",
    "- **Real-time Access**: Access features with low latency for online inference\n",
    "- **Historical Access**: Retrieve point-in-time feature values for training and backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SageMaker Feature Store modules\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.feature_store.feature_definition import FeatureDefinition, FeatureTypeEnum\n",
    "\n",
    "# Initialize SageMaker session\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_region_name\n",
    "s3_bucket_name = session.default_bucket()\n",
    "prefix = \"player-churn-feature-store\"\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"SageMaker session initialized in region: {region}\")\n",
    "print(f\"Using S3 bucket: {s3_bucket_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Feature Store\n",
    "\n",
    "SageMaker Feature Store requires two special columns:\n",
    "1. A **record identifier** column that uniquely identifies each record (we'll use `player_id`)\n",
    "2. An **event time** column that indicates when the feature values were generated\n",
    "\n",
    "We'll add the event time column to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table is available as variable `df`\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def generate_event_timestamp():\n",
    "    # naive datetime representing local time\n",
    "    naive_dt = datetime.now()\n",
    "    # take timezone into account\n",
    "    aware_dt = naive_dt.astimezone()\n",
    "    # time in UTC\n",
    "    utc_dt = aware_dt.astimezone(timezone.utc)\n",
    "    # transform to ISO-8601 format\n",
    "    event_time = utc_dt.isoformat(timespec='milliseconds')\n",
    "    event_time = event_time.replace('+00:00', 'Z')\n",
    "    return event_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'EventTime' with the current timestamp. This will be used as the event time for the feature store.\n",
    "dt = generate_event_timestamp()\n",
    "df_balanced['event_time'] = dt\n",
    "\n",
    "# Define feature group name\n",
    "feature_group_name = \"player-churn-features-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "# Create Feature Group\n",
    "player_churn_feature_group = FeatureGroup(name=feature_group_name, sagemaker_session=session)\n",
    "\n",
    "# Load the data into Feature Store\n",
    "player_churn_feature_group.load_feature_definitions(data_frame=df_balanced)\n",
    "print(f\"Feature group name: {feature_group_name}\")\n",
    "print(\"Feature definitions loaded from dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Feature Group\n",
    "\n",
    "Now we'll create the feature group in SageMaker Feature Store. We'll configure it with:\n",
    "- An S3 location for offline storage\n",
    "- The record identifier column (`player_id`)\n",
    "- The event time column (`EventTime`)\n",
    "- Online store enabled for low-latency access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature group\n",
    "player_churn_feature_group.create(\n",
    "    s3_uri=f\"s3://{s3_bucket_name}/{prefix}\",\n",
    "    record_identifier_name=\"player_id\",\n",
    "    event_time_feature_name=\"event_time\",\n",
    "    role_arn=role,\n",
    "    enable_online_store=True\n",
    ")\n",
    "\n",
    "# Wait for feature group creation to complete\n",
    "# player_churn_feature_group.wait()\n",
    "# print(f\"Feature group {feature_group_name} created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    \"\"\"Helper function to wait for the completions of creating a feature group\"\"\"\n",
    "    response = feature_group.describe()\n",
    "    status = response.get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Feature Group Creation\")\n",
    "        time.sleep(5)\n",
    "        response = feature_group.describe()\n",
    "        status = response.get(\"FeatureGroupStatus\")\n",
    "\n",
    "    if status != \"Created\":\n",
    "        print(f\"Failed to create feature group, response: {response}\")\n",
    "        failureReason = response.get(\"FailureReason\", \"\")\n",
    "        raise SystemExit(\n",
    "            f\"Failed to create feature group {feature_group.name}, status: {status}, reason: {failureReason}\"\n",
    "        )\n",
    "    print(f\"FeatureGroup {feature_group.name} successfully created.\")\n",
    "\n",
    "wait_for_feature_group_creation_complete(feature_group=player_churn_feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Data into Feature Store\n",
    "\n",
    "Finally, we'll ingest our processed data into the feature group. This makes the features available for:\n",
    "- Training new models\n",
    "- Real-time inference\n",
    "- Feature exploration and analysis\n",
    "- Sharing with other teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest data into the feature group\n",
    "player_churn_feature_group.ingest(data_frame=df_balanced, max_workers=3, wait=True)\n",
    "print(f\"Ingested {len(df_balanced)} records into feature group {feature_group_name}\")\n",
    "\n",
    "# Describe the feature group to verify\n",
    "feature_group_details = player_churn_feature_group.describe()\n",
    "print(f\"\\nFeature Group Details:\\n{feature_group_details}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've performed comprehensive feature engineering on the player churn dataset:\n",
    "\n",
    "1. Selected the most relevant features\n",
    "2. Handled missing values\n",
    "3. Applied one-hot encoding to categorical variables\n",
    "4. Balanced the dataset using oversampling\n",
    "5. Converted data types for compatibility with ML algorithms\n",
    "6. Saved the processed data to CSV files\n",
    "7. Stored the features in SageMaker Feature Store for reuse in ML workflows\n",
    "\n",
    "The processed data is now ready for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

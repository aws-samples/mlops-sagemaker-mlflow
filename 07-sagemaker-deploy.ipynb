{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5845c0a-25c6-40e6-a3ba-2b3316bd38c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 4: Add a deployment pipeline\n",
    "In previous four steps you implemented an automated data processing and model building pipeline. Each run of the pipeline produces a new version of the model. This notebook implements the automated model deployment step in our ML workflow.\n",
    "\n",
    "![](img/sagemaker-mlops-project-deploy-diagram.jpg)\n",
    "\n",
    "You can use a [SageMaker MLOps project template](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects-templates.html) to provision a ready-to use model deployment CI/CD pipeline.\n",
    "\n",
    "This template automates the deployment of models in the SageMaker model registry to SageMaker endpoints for real-time inference. This template recognizes changes in the model registry. When a new model version is registered and approved, it automatically initiates a deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ca98f-099f-4f24-8e24-67bdc86d8fc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\"> Make sure you using <code>Python 3</code> kernel in JupyterLab for this notebook.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f78f266-83a4-4172-b1ff-a6bc4712e587",
   "metadata": {},
   "source": [
    "First, we need to install the python dependencies for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58245af4-266c-4c4e-a1e7-a760f2208eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install jsonlines tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd36377-d862-4d18-a575-860bf0c15b31",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker \n",
    "from time import gmtime, strftime, sleep\n",
    "import json\n",
    "import os\n",
    "from sagemaker.predictor import Predictor\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a760443-7627-4029-b01e-2b0f1056566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252d0d80-e858-42d2-84c2-35140c9a3f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b3db4e-4a75-43f3-ab98-96edcd45ac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(model_package_group_name) > 0\n",
    "assert len(region) > 0\n",
    "assert len(bucket_name) > 0\n",
    "assert len(bucket_prefix) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb83a8-521f-4405-bb4f-eee77ab56094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm = boto3.client(\"sagemaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897e41e-e010-4e2c-9df2-f429ccb71be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(project_name)\n",
    "    print(project_id)\n",
    "except NameError:\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"You must complete the notebook 06-sagemaker-project\")\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1908786-a1f2-411e-9019-004e607a9edd",
   "metadata": {},
   "source": [
    "## Explore the project in the Studio UI\n",
    "\n",
    "<div class=\"alert alert-info\">Make sure you run the step 6 notebook and successfully provisioned the MLOps project and run the project pipeline at least once.</div>\n",
    "\n",
    "Click on the links constructed by the next code cell to see the project and the model package in the Studio UI. You must see at least one version of the model registered in the model package group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57957c0e-cb2d-4cd3-8831-30b56f68a430",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check that the project exists\n",
    "project_data = sm.describe_project(ProjectName=project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbc8dec-8ddd-4ab5-86be-f2d61c570ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert project_data['ProjectStatus'] == 'CreateCompleted', 'Project must be created at this point!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb412947-ab49-4856-9129-d8755326127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that at least one model version is registered in the model registry\n",
    "model_packages = sm.list_model_packages(\n",
    "    ModelPackageGroupName=f'{project_name}-{project_id}',\n",
    "    ModelApprovalStatus='PendingManualApproval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1130f2d9-c8a6-4f06-bcef-73158b566dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(model_packages['ModelPackageSummaryList']) > 0, 'You must have at least one model version in the status PendingManualApproval'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceddf72-4a24-447d-9718-d940a3b54252",
   "metadata": {},
   "source": [
    "## Working with MLOps project for model deployment\n",
    "The template provisions a CodeCommit repository with configuration files to specify the model deployment steps, AWS CloudFormation templates to define endpoints as infrastructure, and seed code for testing the endpoint.\n",
    "\n",
    "This template provides the following resources:\n",
    "\n",
    "1. An AWS CodeCommit repository that contains template code that deploys models to endpoints in staging and production environments\n",
    "2. An AWS CodePipeline pipeline that has `source`, `build`, `deploy-to-staging`, and `deploy-to-production` steps. The `source` step points to the CodeCommit repository, and the `build` step gets the code from that repository and generates CloudFormation stacks to deploy. The `deploy-to-staging` and `deploy-to-production` steps deploy the CloudFormation stacks to their respective environments. There is a manual approval step between the staging and production build steps, so that a MLOps engineer must approve the model before it is deployed to production.\n",
    "3. An Amazon EventBridge rule to launch a CodePipeline pipeline execution when a model package version is approved or rejected.\n",
    "4. There is also a manual approval step after the placeholder unit tests. You can implement your own tests to replace the placeholders tests.\n",
    "\n",
    "The template also deploys an Amazon S3 bucket to store artifacts, including CodePipeline and CodeBuild artifacts, and any artifacts generated from the SageMaker pipeline runs.\n",
    "\n",
    "The following diagram shows the architecture.\n",
    "\n",
    "<img src=\"img/mlops-model-deploy.png\" width=\"600\"/>\n",
    "\n",
    "You don't need to implement any configuration changes for the project. The model deployment pipeline works out of the box.\n",
    "To start the model deployment pipeline, you must approve the model version in the model registry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64405ac3-aefa-4ba9-aa8a-f7fca4a78009",
   "metadata": {},
   "source": [
    "### Approve a model version\n",
    "Approving a model version causes the MLOps project to launch the model deployment process. \n",
    "\n",
    "In the first step, the model deployment pipeline deploys the model version to a staging SageMaker real-time inference end-point.\n",
    "\n",
    "You can approve the model version either in Studio in the Model registry or do it programmatically in the notebook. Let's do it programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13610b3-be77-4a2e-abe9-bc0d55acf082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print(model_package_group_name)\n",
    "except NameError:\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"Run the step 03 notebook to create a pipeline, run the pipeline, and register a model version in the model registry\")\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e8925-5e0d-4911-a087-a026b3b1a21f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list all model packages and select the latest one\n",
    "model_packages = []\n",
    "\n",
    "for p in sm.get_paginator('list_model_packages').paginate(\n",
    "        ModelPackageGroupName=model_package_group_name,\n",
    "        SortBy=\"CreationTime\",\n",
    "        SortOrder=\"Descending\",\n",
    "    ):\n",
    "    model_packages.extend(p[\"ModelPackageSummaryList\"])\n",
    "\n",
    "if len(model_packages) == 0:\n",
    "    raise Exception(f\"No model package is found for {model_package_group_name} model package group. Run a model creation pipeline first.\")\n",
    "\n",
    "print(f\"There are {len(model_packages)} model versions in the {model_package_group_name} model package group\")\n",
    "print(f\"Approve the most recent model package:\")\n",
    "\n",
    "latest_model_package_arn = model_packages[0][\"ModelPackageArn\"]\n",
    "print(latest_model_package_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e69876-c48e-441a-adb1-94f37f3c3cc5",
   "metadata": {},
   "source": [
    "The following statement sets the `ModelApprovalStatus` for the most recent model package in the model registry to `Approved`. The model package state change launches the EventBridge rule and the rule launches the CodePipeline CI/CD pipeline with model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef387cb-c889-4e58-a52a-8c7def24836d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_package_update_response = sm.update_model_package(\n",
    "    ModelPackageArn=latest_model_package_arn,\n",
    "    ModelApprovalStatus=\"Approved\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451a3532-845c-4697-b059-330735da6dfd",
   "metadata": {},
   "source": [
    "You can see the last model version in the model registry in the Studio UI changed the **Status** to `Approved`:\n",
    "\n",
    "![](img/model-package-group-version-approval.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca217b12-d020-4eb3-820a-de6431f4e82d",
   "metadata": {},
   "source": [
    "### Deployment pipeline execution\n",
    "Upon approval of a model version in the code cell above, the model deployment CI/CD pipeline performs the following actions:\n",
    "1. Create CloudFormation parameter configuration files with stating and prod parameters for CloudFormation templates with SageMaker endpoint IaC\n",
    "1. Create a SageMaker real-time inference endpoint with the name `<PROJECT-NAME>-staging` in the current account\n",
    "1. Run the test script on the staging endpoint\n",
    "1. Wait until the test result is manually approved in [AWS CodePipeline console](https://console.aws.amazon.com/codesuite/codepipeline)\n",
    "1. Create a SageMaker endpoint with the name `<PROJECT-NAME>-prod` in the current account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3956a712-c8d6-4093-b9d0-e3a5cce443d8",
   "metadata": {},
   "source": [
    "Wait about 10-15 minutes until the pipeline finishes deployment of the staging endpoint. You can see the status of the endpoint in the Studio UI in **Deployments** > **Endpoints**:\n",
    "\n",
    "![](img/sagemaker-mlops-deploy-endpoint-status.jpg)\n",
    "\n",
    "After the endpoint status changed from `Creating` to `InService`, the staging endpoint is fully operational. You can launch the model deployment process to the production stage by manually approving the **DeployStaging** stage of the CodePipeline pipeline. In the next section you approve the model deployment and launch the second stage of the deployment into a production endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eb513c-8171-4c8f-9769-7cae2efbf42a",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <p style=\" text-align: center; margin: auto;\">Wait until staging endpoint status changes to InService, then continue with the following code cells.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a955a06c-1ce7-40d8-84a6-a0c522da288e",
   "metadata": {},
   "source": [
    "# Testing the SageMaker endpoint\n",
    "After a successful deployment to a SageMaker endpoint in staging, let's verify the endpoint by running some inferences through it.\n",
    "When it comes to serving your model in an endpoint, Sagemaker offers many different options:\n",
    "\n",
    "## SageMaker Endpoint\n",
    "SageMaker provides different options for your inference use cases, giving you choice over the technical breadth and depth of your deployments:\n",
    "\n",
    "* **Deploying a model to an endpoint.** When deploying your model, consider the following options:\n",
    "   + [Real-time inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html). Real-time inference is ideal for inference workloads where you have interactive, low latency requirements.\n",
    "   + [Deploy models with Amazon SageMaker Serverless Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html). Use Serverless Inference to deploy models without configuring or managing any of the underlying infrastructure. This option is ideal for workloads which have idle periods between traffic spurts and can tolerate cold starts.\n",
    "   + [Asynchronous inference](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html). queues incoming requests and processes them asynchronously. This option is ideal for requests with large payload sizes (up to 1GB), long processing times (up toAsynchronous Inference one hour), and near real-time latency requirements\n",
    "\n",
    "* **Cost optimization**. To optimize your inference costs, consider the following options:\n",
    "\n",
    "   + [Automatically Scale Amazon SageMaker Models](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html). Use autoscaling to dynamically adjust the compute resources for your endpoints based on incoming traffic patterns, which helps you optimize costs by only paying for the resources you're using at a given time.\n",
    " \n",
    "The following diagram provides an overview of all the deployment options in SageMaker:\n",
    "\n",
    "![](img/sagemaker-deployment-modes.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9c7834-e18e-41a6-ba57-d4b4077f9a08",
   "metadata": {},
   "source": [
    "## Real time Inference from a SageMaker endpoint\n",
    "To demonstrate the inference capabilities, we'll explore both realtime and batch transform in this notebook. We'll use the staging endpoint for our test so that we know the endpoint works as expected before deploying to production environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a804bf5-d01c-41b9-b2bd-066ee46daf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all deployed real-time endpoints\n",
    "endpoints = sm.list_endpoints(StatusEquals=\"InService\")[\"Endpoints\"]\n",
    "\n",
    "if not len(endpoints):\n",
    "    print(\"There are no deployed active endpoints. You must have at least one endpoint. Run the previous cell in this notebook to deploy an endpoint\")\n",
    "else:\n",
    "    print(f\"Found {len(endpoints)} active inference endpoint(s):\\n\")\n",
    "    \n",
    "    for i, endpoint in enumerate(endpoints, 1):\n",
    "        print(f\"{i}. Endpoint Name: {endpoint['EndpointName']}\")\n",
    "        print(f\"   Status: {endpoint['EndpointStatus']}\")\n",
    "        print(f\"   Creation Time: {endpoint['CreationTime']}\")\n",
    "        print(f\"   Last Modified: {endpoint['LastModifiedTime']}\")\n",
    "        print(\"---\")\n",
    "    \n",
    "    # If you still need to select one endpoint for further operations, \n",
    "    # you can use the first one or add selection logic:\n",
    "    endpoint_name = endpoints[0]['EndpointName']\n",
    "    print(f\"\\nUsing endpoint '{endpoint_name}' for subsequent operations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99736857-8e8b-4826-afa8-0cc3dcbd671e",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> If you are running this workshop as an instructor led training, the `endpoint_name` is set for you so there is nothing for you to do. Otherwise, simply update the endpoint_name with the appropriate endpoint_name for the test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6afc2d-7dfa-4b90-ac30-851f1f09d946",
   "metadata": {},
   "source": [
    "### Define helper functions\n",
    "Define some helper functions with code snippets that you're going to use throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f768a-6acb-411f-bb35-f02030918b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send data to the endpoint\n",
    "def realtime_prediction(predictor, data):\n",
    "    l = len(data)\n",
    "    for i in trange(l):\n",
    "        data_arr = [float(np_float) for np_float in data.iloc[i].values ]\n",
    "        predictions = np.array(predictor.predict(data_arr), dtype=float).squeeze()\n",
    "        print(predictions)\n",
    "\n",
    "def download_from_s3(s3_client, local_file_path, bucket_name, s3_file_path):\n",
    "    try:\n",
    "        # Download the file\n",
    "        s3_client.download_file(bucket_name, s3_file_path, local_file_path)\n",
    "        print(f\"File downloaded successfully to {local_file_path}\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            print(\"The object does not exist.\")\n",
    "        else:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return False\n",
    "\n",
    "def upload_to_s3(s3_client, local_file_path, bucket_name, s3_file_path=None):\n",
    "    # If S3 file path is not specified, use the basename of the local file\n",
    "    if s3_file_path is None:\n",
    "        s3_file_path = os.path.basename(local_file_path)\n",
    "\n",
    "    try:\n",
    "        # Upload the file\n",
    "        s3_client.upload_file(local_file_path, bucket_name, s3_file_path)\n",
    "        print(f\"File {local_file_path} uploaded successfully to {bucket_name}/{s3_file_path}\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"ClientError: {e}\")\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {local_file_path} was not found\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return False\n",
    "        \n",
    "def write_params(s3_client, step_name, params, notebook_param_s3_bucket_prefix):\n",
    "    local_file_path = f\"{step_name}.json\"\n",
    "    with open(local_file_path, \"w\") as f:\n",
    "        f.write(json.dumps(params))\n",
    "    base_local_file_path = os.path.basename(local_file_path)\n",
    "    bucket_name = notebook_param_s3_bucket_prefix.split(\"/\")[2] # Format: s3://<bucket_name>/..\n",
    "    s3_file_path = os.path.join(\"/\".join(notebook_param_s3_bucket_prefix.split(\"/\")[3:]), base_local_file_path)\n",
    "    upload_to_s3(s3_client, local_file_path, bucket_name, s3_file_path)\n",
    "    \n",
    "def read_params(s3_client, notebook_param_s3_bucket_prefix, step_name):\n",
    "    local_file_path = f\"{step_name}.json\"\n",
    "    base_local_file_path = os.path.basename(local_file_path)\n",
    "    bucket_name = notebook_param_s3_bucket_prefix.split(\"/\")[2] # Format: s3://<bucket_name>/..\n",
    "    s3_file_path = os.path.join(\"/\".join(notebook_param_s3_bucket_prefix.split(\"/\")[3:]),  base_local_file_path)\n",
    "    downloaded = download_from_s3(s3_client, local_file_path, bucket_name, s3_file_path)\n",
    "    with open(local_file_path, \"r\") as f:\n",
    "        data = f.read()\n",
    "        params = json.loads(data)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c6395f-943c-403a-a516-9b90ff5535e0",
   "metadata": {},
   "source": [
    "### Generate realtime prediction based on test data\n",
    "In the following cell, we'll run some inferences in realtime using the test data captured in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0958291-d056-4fa7-924f-09f7a6fe7c61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a predictor class for the endpoint\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name, \n",
    "    serializer=sagemaker.serializers.CSVSerializer(),\n",
    "    deserializer=sagemaker.deserializers.CSVDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe1df4e-8559-498f-92f0-c11550772396",
   "metadata": {},
   "source": [
    "In [02-preprocess.ipynb](02-preprocess.ipynb) we divided the dataset into training, validation and test dataset. For this lab, we'll use the test dataset for running inferences against the deployed endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba1f94-fc4b-4f01-955f-edf2bd8b098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_step_name = \"02-preprocess\"\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "notebook_param_s3_bucket_prefix=f\"s3://{bucket_name}/{bucket_prefix}/params\"\n",
    "preprocess_step_params = read_params(s3_client, notebook_param_s3_bucket_prefix, preprocess_step_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685297ca-d9e3-4956-b7dd-85ef3b7425ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_test_x_file_path = \"test_x.csv\"\n",
    "local_test_y_file_path = \"test_y.csv\"\n",
    "s3_test_x_data = preprocess_step_params[\"test_x_data\"]\n",
    "s3_test_y_data = preprocess_step_params[\"test_y_data\"]\n",
    "\n",
    "# Download the test_x.csv and test_y.csv file from S3\n",
    "bucket_name = s3_test_x_data.split(\"/\")[2]\n",
    "s3_test_x_data_key = \"/\".join(s3_test_x_data.split(\"/\")[3:])\n",
    "s3_test_y_data_key = \"/\".join(s3_test_y_data.split(\"/\")[3:])\n",
    "download_from_s3(s3_client, local_test_x_file_path, bucket_name, s3_test_x_data_key)\n",
    "download_from_s3(s3_client, local_test_y_file_path, bucket_name, s3_test_y_data_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d50a74a-da89-442c-8788-6ced63706d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of data vectors from the test dataset sent to the inference endpoint as batch\n",
    "number_of_vectors = 10\n",
    "test_x = pd.read_csv(\"test_x.csv\", header=None).sample(number_of_vectors)\n",
    "\n",
    "# Select only the first 10 features (columns 0-9) to match the trained model\n",
    "test_x = test_x.iloc[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b9504f-faf7-4072-bc37-0905c258e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the output to see the response payload\n",
    "print(f\"Test data shape after feature selection: {test_x.shape}\")\n",
    "test_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84f1848-94d9-4d23-b66c-3325fec432da",
   "metadata": {},
   "source": [
    "Rename the column names for identifying the feature attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60390899-be22-4720-b24e-aa3c4ef4aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_x will have 10 columns\n",
    "test_x.columns = [f'_c{i}' for i in range(len(test_x.columns))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07523904-416f-44c3-ac4b-7597cac7db7a",
   "metadata": {},
   "source": [
    "Run prediction using the realtime endpoint deployed in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef97f47-f450-4b62-8ed1-e7666845d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "realtime_prediction(predictor, test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cf3aaa-ec30-4bb9-9972-206e6de02c5f",
   "metadata": {},
   "source": [
    "# Batch Transform\n",
    "SageMaker offers [batch transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) to optimize inference workloads for the following  scenarios:\n",
    "\n",
    "* Preprocess datasets to remove noise or bias that interferes with training or inference from your dataset.\n",
    "* Get inferences from large datasets.\n",
    "* Run inference when you don't need a persistent endpoint.\n",
    "* Associate input records with inferences to help with the interpretation of results.\n",
    "\n",
    "Functionally, batch transform uses the same mechanics as real-time hosting to generate predictions. It requires a web server that takes in HTTP POST requests a single observation, or mini-batch, at a time. However, unlike real-time hosted endpoints which have persistent hardware (instances stay running until you shut them down), batch transform clusters are torn down when the job completes.\n",
    "\n",
    "To demonstrate the capability, we'll run a batch transform job on the same dataset that we used for realtime inference previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcfe942-1e6e-4b4e-bc66-f527432a6439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and preprocess the test data to use only first 10 features\n",
    "import pandas as pd\n",
    "\n",
    "# Download the original test data\n",
    "local_test_x_batch_file = \"test_x_batch_full.csv\"\n",
    "download_from_s3(s3_client, local_test_x_batch_file, bucket_name, s3_test_x_data_key)\n",
    "\n",
    "# Load and select only first 10 features\n",
    "test_x_full = pd.read_csv(local_test_x_batch_file, header=None)\n",
    "test_x_batch = test_x_full.iloc[:, :10]  # Select only first 10 features\n",
    "\n",
    "print(f\"Original batch data shape: {test_x_full.shape}\")\n",
    "print(f\"Modified batch data shape: {test_x_batch.shape}\")\n",
    "\n",
    "# Save the modified test data locally\n",
    "local_test_x_batch_processed = \"test_x_batch_processed.csv\"\n",
    "test_x_batch.to_csv(local_test_x_batch_processed, header=False, index=False)\n",
    "\n",
    "# Upload the processed data to S3\n",
    "s3_test_x_batch_processed_key = f\"{bucket_prefix}/test/test_x_batch_processed.csv\"\n",
    "s3_test_x_batch_processed_path = f\"s3://{bucket_name}/{s3_test_x_batch_processed_key}\"\n",
    "\n",
    "s3_client.upload_file(\n",
    "    local_test_x_batch_processed, \n",
    "    bucket_name, \n",
    "    s3_test_x_batch_processed_key\n",
    ")\n",
    "\n",
    "print(f\"Processed batch data uploaded to: {s3_test_x_batch_processed_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40946cab-65db-4774-91ad-cae8607047e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.inputs import TransformInput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a407b0-1808-4470-851f-e11c866045f9",
   "metadata": {},
   "source": [
    "First we'll get the model_name from the deployed sagemaker endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662428fb-61fb-48e6-929e-a0302ab3fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm.describe_endpoint(\n",
    "    EndpointName=endpoint_name\n",
    ")\n",
    "endpoint_config_name = response[\"EndpointConfigName\"]\n",
    "response = sm.describe_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "model_name = response['ProductionVariants'][0]['ModelName']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d6225-6b28-4e31-ac60-5eeb7dc95a7e",
   "metadata": {},
   "source": [
    "Define batch_transform variables to use for running a batch tranform job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd8f662-53b9-40a4-8818-406d72b32a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_transform_instance_type = \"ml.m5.large\"\n",
    "batch_transform_output_path = f\"s3://{bucket_name}/{bucket_prefix}/transform\"\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a55073-1bee-45a1-93fa-c86fbb678dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the transform step\n",
    "transformer = Transformer(\n",
    "        model_name=model_name,\n",
    "        instance_type=batch_transform_instance_type,\n",
    "        instance_count=1,\n",
    "        accept=\"text/csv\",\n",
    "        assemble_with=\"Line\",\n",
    "        output_path=batch_transform_output_path,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        base_transform_job_name=f\"player-churn-model-batch-transform\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d6ea6c-32a5-4780-b2e4-789e043ef7a7",
   "metadata": {},
   "source": [
    "Trigger a batch tranform job using the SageMaker python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa19d50-6091-400f-9f41-977257172ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the processed data with only 10 features instead of original data\n",
    "transformer.transform(    \n",
    "        data=s3_test_x_batch_processed_path,  # Changed from s3_test_x_data\n",
    "        content_type=\"text/csv\",\n",
    "        split_type=\"Line\", \n",
    "        join_source=\"Input\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c099aaf3-3f0e-42d8-83cc-ca6bb14d3e98",
   "metadata": {},
   "source": [
    "Download the inference results from the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12ba181-abb2-4f43-baf5-7fff61a6a261",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive $transformer.output_path ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baab355-500f-4bd0-90f5-887c11a0177e",
   "metadata": {},
   "source": [
    "Let's take a look at the payload. The predicted value "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3050ac11-bfe5-4c06-a547-d0c9d81a9f30",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> The results above shows the response payload that contains the input data and the predictions in CSV format. The payload contains both the input and the predicted label. In addition to the default output structure, you can also customize the way batch transform constructs the output. Please refer to this [link](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform-data-processing.html#batch-transform-data-processing-examples) to learn more about these customizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7307c5-a234-4fe1-8d32-2c43d804d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -10 test_x_batch_processed.csv.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579140af-5441-47d9-bf54-da9a01b851e6",
   "metadata": {},
   "source": [
    "### Deploy the model version to production\n",
    "Now that we've validated the staging endpoint, assuming we are happy with the results. Next, we'll continue the deployment pipeline to production deployment. \n",
    "\n",
    "Let's construct a CodePipeline approval link. \n",
    "\n",
    "If you used the option 1 `boto3` to create an MLOps project, the `project_name` and `project_id` are set automatically. You can run the following code cell to print the values. If you followed the UI instructions to create a project, you must set the `project_name` manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa30c033-daa8-4ab5-899c-aed5f8f4d9af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print(project_name)\n",
    "    print(project_id)\n",
    "except NameError:\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"You must set the project_name manually\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29598cf-3c99-440a-886f-6045605d7f97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set to the model deployment project name if you didn't use boto3-based deployment\n",
    "# project_name = \"<PROJECT_NAME>\"\n",
    "\n",
    "# Get project id\n",
    "project_id = sm.describe_project(ProjectName=project_name)['ProjectId']\n",
    "\n",
    "# Construct the CodePipline pipeline name\n",
    "code_pipeline_name = f\"sagemaker-{project_name}-{project_id}-modeldeploy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa1cfc-7e2c-4a05-b354-d1313fbb7b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    " # approve the latest model version\n",
    "model_package_update_response = sm.update_model_package(\n",
    "    ModelPackageArn=latest_model_package_arn,\n",
    "    ModelApprovalStatus=\"Approved\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d199ea-5ec0-441a-b58f-be85b9a4788b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "# Show the approval link\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Please approve the manual step in <a target=\"top\" href=\"https://console.aws.amazon.com/codesuite/codepipeline/pipelines/{}/view?region={}\">AWS CodePipeline</a></b>'.format(\n",
    "            code_pipeline_name, region)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e825c8b7-77e4-4929-9b73-396502f9d83a",
   "metadata": {},
   "source": [
    "Click on the link ^^^ above ^^^ to open the CodePipeline console with the pipeline execution workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87e002e-e776-4c73-adcc-5ea79d937579",
   "metadata": {},
   "source": [
    "In the **DeployStaging stage**, choose **Review** on the **ApproveDeployment** step. Note, you might wait until `TestStaging` step completes with `Succeeded` status. \n",
    "\n",
    "![](img/deploy-staging-review.png)\n",
    "\n",
    "In the **Review** dialog box, select **Approve** and choose **Submit**:\n",
    "\n",
    "![](img/approve-deployment.png)\n",
    "\n",
    "Approving the **DeployStaging** stage causes the deployment pipeline to continue and to deploy the model to the production endpoint. To view the endpoints, choose the **Deployments** > **Endpoints** in Studio UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39494560-27a0-49af-92b9-1caafb7c34ae",
   "metadata": {},
   "source": [
    "As your CI/CD deployment pipeline continues, you see the production endpoint in status `Creating` along with the previously deployed staging endpoint in status `InService`:\n",
    "\n",
    "![](img/endpoint-prod-creating.png)\n",
    "\n",
    "After `10-15` min the deployment is completed and both endpoints are in status `InService`.\n",
    "\n",
    "Navigate to the Studio and choose **Deployments** > **Projects**. In the Project pane select `model-deploy-<TIMESTAMP>` project. In the project details pane select **Endpoints**. You see that both endpoints, `staging` and `prod`, are visible in the deployment project because the project and the endpoints are connected via the metadata:\n",
    "\n",
    "![](img/project-endpoints.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ace97-ba05-44bc-bc36-38df670fafb9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook you implemented an automated CI/CD deployment pipeline with the following features:\n",
    "- use CloudFormation IaC templates for SageMaker real-time inference endpoint deployment\n",
    "- model approval in the model registry launches the model deployment pipeline\n",
    "- model deployment pipeline contains two stages, staging and production with automated tests for the staging endpoint and manual approval for the production deployment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a45818-0bb7-48b1-bfc6-3c701f6eaa63",
   "metadata": {},
   "source": [
    "## Clean-up\n",
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <p style=\" text-align: center; margin: auto;\">\n",
    "    If you're going to run the step 6 notebook (Data and Model Quality Monitoring), you need to keep at least one of the endpoints. If you finish the workshop here and don't run the step 6 notebook, navigate to the <b>clean-up notebook (99-clean-up.ipynb)</b> and follow the clean-up instructions to avoid charges in your AWS account.\n",
    "    <br>\n",
    "    <br>\n",
    "    You don't need to run the clean-up if you're using an AWS-provided AWS account.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b9d484-5022-4816-8a1b-89e27ffadc66",
   "metadata": {},
   "source": [
    "## Further development ideas for your real-world projects\n",
    "- Add end-to-end data encryption using AWS KMS keys\n",
    "- Create a [custom SageMaker project template](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects-templates-custom.html) for model deployment to cover your specific project requirements\n",
    "- Add [multi-account model deployment](https://aws.amazon.com/blogs/machine-learning/multi-account-model-deployment-with-amazon-sagemaker-pipelines/) to your ML workflow\n",
    "- Add automated model tests to the placeholder in the CodePipeline pipeline\n",
    "- Use [Amazon SageMaker Inference Recommender](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html) to run automated load tests for your inference endpoints and to select the best instance type and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbed7d41-921d-4174-a177-33c3acc95049",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "- [Deploy a Machine Learning Model to a Real-Time Inference Endpoint](https://aws.amazon.com/getting-started/hands-on/machine-learning-tutorial-deploy-model-to-real-time-inference-endpoint/)\n",
    "- [SageMaker MLOps Project Walkthrough](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects-walkthrough.html)\n",
    "- [Amazon SageMaker Pipelines lab in SageMaker Immersion Day](https://catalog.us-east-1.prod.workshops.aws/workshops/63069e26-921c-4ce1-9cc7-dd882ff62575/en-US/lab6)\n",
    "- [Amazon SageMaker secure MLOps](https://github.com/aws-samples/amazon-sagemaker-secure-mlops)\n",
    "- [Testing approaches for Amazon SageMaker ML models](https://aws.amazon.com/blogs/machine-learning/testing-approaches-for-amazon-sagemaker-ml-models/)\n",
    "- [Model hosting patterns in Amazon SageMaker blog series](https://aws.amazon.com/blogs/machine-learning/model-hosting-patterns-in-amazon-sagemaker-part-1-common-design-patterns-for-building-ml-applications-on-amazon-sagemaker/)\n",
    "- [Take advantage of advanced deployment strategies using Amazon SageMaker deployment guardrails](https://aws.amazon.com/blogs/machine-learning/take-advantage-of-advanced-deployment-strategies-using-amazon-sagemaker-deployment-guardrails/)\n",
    "- [MLOps deployment best practices for real-time inference model serving endpoints with Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/mlops-deployment-best-practices-for-real-time-inference-model-serving-endpoints-with-amazon-sagemaker/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca846f6c-3645-493a-a923-7457b2e32a5f",
   "metadata": {},
   "source": [
    "# Shutdown kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60e56c7-b568-4208-818c-42634870f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
